{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Excel Files in Python\n",
    "\n",
    "# =============================================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Luis Paulo Vinatea Barberena  \n",
    "**Date:** 2025-05-21\n",
    "\n",
    "When you need to work with Excel files in Python, you can use the `openpyxl` library. This library allows you to read and write Excel files in the `.xlsx` format.\n",
    "It is a powerful tool for data manipulation and analysis, especially when dealing with large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================================================================================\n",
    "\n",
    "## Import Standard Libraries\n",
    "\n",
    "# ========================================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import openpyxl\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "# Import project utilities\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from src.utils.data_processing import *  # noqa: F403\n",
    "from src.utils.visualization import *  # noqa: F403\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "XLSX_DATA_PATH = \"../../data/raw/*.xlsx\"\n",
    "OUTPUT_DIR = \"../../data/processed/sites/df1/\"\n",
    "\n",
    "# Global visualization settings\n",
    "used_colors = set()\n",
    "site_color_mapping = {}\n",
    "available_css4_colors = list(mcolors.CSS4_COLORS.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "\n",
    "## Data Loading Functions\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATA\n",
      "============================================================\n",
      "Loading data from ../../data/raw/kpis_ambientales_2025.xlsx\n",
      "\n",
      "Workbook: kpis_ambientales_2025.xlsx\n",
      "Contains 4 sheets: DATOS, Huella 2025, Calculo de reduccion, INDICADORES\n",
      "\n",
      "  Sheet: 'DATOS'\n",
      "  Dimensions: 57 rows x 8 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 4\n",
      "  Loaded df1 with shape (53, 8)\n",
      "\n",
      "  Sheet: 'Huella 2025'\n",
      "  Dimensions: 21 rows x 19 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 6\n",
      "  Loaded df2 with shape (15, 19)\n",
      "\n",
      "  Sheet: 'Calculo de reduccion'\n",
      "  Dimensions: 9 rows x 8 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 3\n",
      "  Loaded df3 with shape (6, 8)\n",
      "\n",
      "  Sheet: 'INDICADORES'\n",
      "  Dimensions: 14 rows x 9 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 5\n",
      "  Loaded df4 with shape (9, 9)\n",
      "Loading data from ../../data/raw/objetivos_y_estrategias_2025.xlsx\n",
      "\n",
      "Workbook: objetivos_y_estrategias_2025.xlsx\n",
      "Contains 1 sheets: Hoja1\n",
      "\n",
      "  Sheet: 'Hoja1'\n",
      "  Dimensions: 38 rows x 11 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 5\n",
      "  Loaded df5 with shape (33, 11)\n",
      "\n",
      "Successfully loaded 5 dataframes.\n"
     ]
    }
   ],
   "source": [
    "def load_excel_files(data_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load all Excel files from the specified path and create DataFrames.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path pattern for Excel files\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing loaded DataFrames\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    df_count = 1\n",
    "\n",
    "    try:\n",
    "        for file in glob.glob(data_path):\n",
    "            print(f\"Loading data from {file}\")\n",
    "\n",
    "            # Load workbook with openpyxl for inspection\n",
    "            wb = openpyxl.load_workbook(file, data_only=True, read_only=True)\n",
    "            file_basename = os.path.basename(file)\n",
    "\n",
    "            print(f\"\\nWorkbook: {file_basename}\")\n",
    "            print(\n",
    "                f\"Contains {len(wb.sheetnames)} sheets: {', '.join(wb.sheetnames)}\"\n",
    "            )\n",
    "\n",
    "            # Process each sheet\n",
    "            for sheet_name in wb.sheetnames:\n",
    "                df_name = f\"df{df_count}\"\n",
    "                df = _load_sheet_with_header_detection(\n",
    "                    file, sheet_name, wb[sheet_name]\n",
    "                )\n",
    "\n",
    "                if df is not None:\n",
    "                    dfs[df_name] = df\n",
    "                    print(f\"  Loaded {df_name} with shape {df.shape}\")\n",
    "                    df_count += 1\n",
    "\n",
    "            wb.close()\n",
    "\n",
    "        print(f\"\\nSuccessfully loaded {df_count - 1} dataframes.\")\n",
    "        return dfs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def _load_sheet_with_header_detection(\n",
    "    file_path: str, sheet_name: str, sheet\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a sheet with automatic header detection.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "        sheet_name (str): Name of the sheet\n",
    "        sheet: Openpyxl sheet object\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame or None if sheet is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if sheet is empty\n",
    "        if not sheet.max_row or not sheet.max_column:\n",
    "            print(\n",
    "                f\"  Warning: Sheet '{sheet_name}' appears to be empty or corrupted\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        print(f\"\\n  Sheet: '{sheet_name}'\")\n",
    "        print(\n",
    "            f\"  Dimensions: {sheet.max_row} rows x {sheet.max_column} columns\"\n",
    "        )\n",
    "\n",
    "        # Check for merged cells\n",
    "        try:\n",
    "            merged_cells = list(sheet.merged_cells.ranges)\n",
    "            if merged_cells:\n",
    "                print(f\"  Contains {len(merged_cells)} merged cell ranges\")\n",
    "        except AttributeError:\n",
    "            print(\"  Note: Cannot check merged cells in read-only mode\")\n",
    "\n",
    "        # Load initial DataFrame without headers\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            na_values=[\"NA\", \"N/A\", \"\"],\n",
    "            keep_default_na=True,\n",
    "        )\n",
    "\n",
    "        # Detect header row\n",
    "        header_row = _detect_header_row(df)\n",
    "\n",
    "        if header_row is not None:\n",
    "            # Reload with detected header\n",
    "            df = pd.read_excel(\n",
    "                file_path,\n",
    "                sheet_name=sheet_name,\n",
    "                header=header_row,\n",
    "                na_values=[\"NA\", \"N/A\", \"\"],\n",
    "                keep_default_na=True,\n",
    "            )\n",
    "            print(f\"  Detected header at row {header_row + 1}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading sheet '{sheet_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _detect_header_row(df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to analyze\n",
    "\n",
    "    Returns:\n",
    "        int: Index of the header row, or None if not found\n",
    "    \"\"\"\n",
    "    for i in range(min(10, len(df))):\n",
    "        str_count = sum(1 for x in df.iloc[i] if isinstance(x, str))\n",
    "        if str_count > 0.5 * df.shape[1]:  # More than half are strings\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "# Step 1: Load data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\" * 60)\n",
    "dfs = load_excel_files(XLSX_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "\n",
    "## Data Cleaning Functions\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: CLEANING DATA\n",
      "============================================================\n",
      "Starting data cleaning process...\n",
      "==================================================\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "==================================================\n",
      "\n",
      "Processing df1...\n",
      "  Found 2 unnamed columns\n",
      "  Standardizing 4 date-related columns to Spanish format\n",
      "Processing df2...\n",
      "  Found 7 unnamed columns\n",
      "Processing df3...\n",
      "  Found 1 unnamed columns\n",
      "Processing df4...\n",
      "  Found 2 unnamed columns\n",
      "  Found potential header values in row 1\n",
      "  Renaming 1 columns\n",
      "  New columns: ['variaci贸n porcentual mensual del consumo']\n",
      "Processing df5...\n",
      "\n",
      "Header standardization complete!\n",
      "Cleaned df1: (53, 8) -> (43, 7)\n",
      "Cleaned df2: (15, 19) -> (14, 10)\n",
      "Cleaned df3: (6, 8) -> (6, 7)\n",
      "Cleaned df4: (8, 9) -> (5, 8)\n",
      "Cleaned df5: (33, 11) -> (33, 11)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for df1:\n",
      "- Total NaN values: 82 (27.2% of all cells)\n",
      "- Columns with >80% NaNs: Unnamed: 3\n",
      "- Critical columns: STIOS, INDICADOR, Enero, Febrero, Marzo, Abril\n",
      "- Found 0 rows where all critical columns are NaN\n",
      "- Final shape after analysis: (43, 6)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for df2:\n",
      "- Total NaN values: 24 (17.1% of all cells)\n",
      "- Columns with >80% NaNs: Unnamed: 18\n",
      "- Critical columns: Enero, Febrero, marzo, abril\n",
      "- Found 2 rows where all critical columns are NaN\n",
      "- Final shape after analysis: (12, 9)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for df3:\n",
      "- Total NaN values: 1 (2.4% of all cells)\n",
      "- Critical columns: Material reciclado, Enero, Febrero, marzo, Abril, Emisiones evitadas (aproximado), TOTAL\n",
      "- Found 0 rows where all critical columns are NaN\n",
      "- Final shape after analysis: (6, 7)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for df4:\n",
      "- Total NaN values: 15 (37.5% of all cells)\n",
      "- Critical columns: Indicador, variaci贸n porcentual mensual del consumo, Frecuencia , Formula , Enero, Febrero, Marzo, Aril\n",
      "- Found 0 rows where all critical columns are NaN\n",
      "- Final shape after analysis: (5, 8)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for df5:\n",
      "- Total NaN values: 241 (66.4% of all cells)\n",
      "- Columns with >80% NaNs: quien, cuando, como \n",
      "- Critical columns: Iten, Origen, Objetivo, Meta, Criterios Evaluaci贸n, Responsable, Estrateg铆a \"que\", Acciones Realizadas\n",
      "- Found 0 rows where all critical columns are NaN\n",
      "- Final shape after analysis: (33, 8)\n",
      "Data cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframes(dfs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Apply comprehensive cleaning to all DataFrames.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning process...\")\n",
    "\n",
    "    # Apply cleaning steps in order\n",
    "    dfs = standardize_headers(dfs)\n",
    "    dfs = remove_empty_rows_and_cols(dfs)\n",
    "    dfs = handle_nan_values(dfs)\n",
    "    dfs = title_case_columns(dfs)\n",
    "\n",
    "    print(\"Data cleaning completed!\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def standardize_headers(dfs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Standardize column headers across all DataFrames.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with standardized headers\n",
    "    \"\"\"\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(\"STANDARDIZING DATAFRAME HEADERS\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"Processing {df_name}...\")\n",
    "\n",
    "        # Handle unnamed columns\n",
    "        df_obj = _fix_unnamed_columns(df_obj)\n",
    "\n",
    "        # Standardize date columns\n",
    "        df_obj = _standardize_date_columns(df_obj)\n",
    "\n",
    "        dfs[df_name] = df_obj\n",
    "\n",
    "    print(\"\\nHeader standardization complete!\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def _fix_unnamed_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fix unnamed columns by finding header values in data rows.\"\"\"\n",
    "    unnamed_cols = [col for col in df.columns if \"Unnamed\" in str(col)]\n",
    "\n",
    "    if len(unnamed_cols) > 0:\n",
    "        print(f\"  Found {len(unnamed_cols)} unnamed columns\")\n",
    "\n",
    "        for i in range(min(5, len(df))):\n",
    "            row = df.iloc[i]\n",
    "            str_values = [v for v in row if isinstance(v, str) and pd.notna(v)]\n",
    "\n",
    "            if len(str_values) >= len(unnamed_cols) * 0.7:\n",
    "                new_names = {}\n",
    "                original_cols = df.columns.tolist()\n",
    "\n",
    "                for j, col in enumerate(original_cols):\n",
    "                    if (\n",
    "                        \"Unnamed\" in str(col)\n",
    "                        and j < len(row)\n",
    "                        and pd.notna(row.iloc[j])\n",
    "                        and isinstance(row.iloc[j], str)\n",
    "                    ):\n",
    "                        new_names[col] = str(row.iloc[j]).strip()\n",
    "\n",
    "                if new_names:\n",
    "                    print(f\"  Found potential header values in row {i + 1}\")\n",
    "                    print(f\"  Renaming {len(new_names)} columns\")\n",
    "\n",
    "                    df = df.rename(columns=new_names)\n",
    "                    df = df.drop(index=i)\n",
    "                    print(f\"  New columns: {list(new_names.values())}\")\n",
    "                    break\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _standardize_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize date column names to Spanish format.\"\"\"\n",
    "    date_mappings = {\n",
    "        \"January\": \"Enero\",\n",
    "        \"February\": \"Febrero\",\n",
    "        \"March\": \"Marzo\",\n",
    "        \"April\": \"Abril\",\n",
    "        \"May\": \"Mayo\",\n",
    "        \"June\": \"Junio\",\n",
    "        \"July\": \"Julio\",\n",
    "        \"August\": \"Agosto\",\n",
    "        \"September\": \"Septiembre\",\n",
    "        \"October\": \"Octubre\",\n",
    "        \"November\": \"Noviembre\",\n",
    "        \"December\": \"Diciembre\",\n",
    "    }\n",
    "\n",
    "    date_cols = [col for col in df.columns if str(col) in date_mappings]\n",
    "    if date_cols:\n",
    "        print(\n",
    "            f\"  Standardizing {len(date_cols)} date-related columns to Spanish format\"\n",
    "        )\n",
    "        df = df.rename(columns=date_mappings)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_empty_rows_and_cols(dfs: dict) -> dict:\n",
    "    \"\"\"Remove completely empty rows and columns from all DataFrames.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        original_shape = df_obj.shape\n",
    "        df_obj = df_obj.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        dfs[df_name] = df_obj\n",
    "        print(f\"Cleaned {df_name}: {original_shape} -> {df_obj.shape}\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def handle_nan_values(dfs: dict) -> dict:\n",
    "    \"\"\"Analyze and selectively handle NaN values in DataFrames.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"\\n{'-' * 50}\")\n",
    "        print(f\"NaN analysis for {df_name}:\")\n",
    "\n",
    "        # Calculate NaN statistics\n",
    "        total_cells = df_obj.shape[0] * df_obj.shape[1]\n",
    "        nan_count = df_obj.isna().sum().sum()\n",
    "        nan_percentage = (nan_count / total_cells) if total_cells > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"- Total NaN values: {nan_count} ({nan_percentage:.1%} of all cells)\"\n",
    "        )\n",
    "\n",
    "        # Handle high-NaN columns\n",
    "        df_obj = _remove_high_nan_columns(df_obj)\n",
    "\n",
    "        # Handle critical row removal\n",
    "        df_obj = _remove_critical_nan_rows(df_obj)\n",
    "\n",
    "        dfs[df_name] = df_obj\n",
    "        print(f\"- Final shape after analysis: {df_obj.shape}\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def _remove_high_nan_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove columns with >80% NaN values.\"\"\"\n",
    "    column_nan_pct = df.isna().mean().sort_values(ascending=False)\n",
    "    high_nan_cols = column_nan_pct[column_nan_pct > 0.8].index.tolist()\n",
    "\n",
    "    if high_nan_cols:\n",
    "        print(f\"- Columns with >80% NaNs: {', '.join(high_nan_cols)}\")\n",
    "        df = df.drop(columns=high_nan_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _remove_critical_nan_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows where critical columns are all NaN.\"\"\"\n",
    "    critical_cols = [col for col in df.columns if \"Unnamed\" not in str(col)]\n",
    "\n",
    "    if not critical_cols and len(df.columns) > 2:\n",
    "        critical_cols = [df.columns[1], df.columns[2]]\n",
    "\n",
    "    if critical_cols:\n",
    "        print(f\"- Critical columns: {', '.join(map(str, critical_cols))}\")\n",
    "        rows_to_drop = df[df[critical_cols].isna().all(axis=1)].index\n",
    "        print(\n",
    "            f\"- Found {len(rows_to_drop)} rows where all critical columns are NaN\"\n",
    "        )\n",
    "        df = df.drop(index=rows_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def title_case_columns(dfs: dict) -> dict:\n",
    "    \"\"\"Apply title case to all column names.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        df_obj.columns = df_obj.columns.str.title()\n",
    "        dfs[df_name] = df_obj\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# Step 2: Clean data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: CLEANING DATA\")\n",
    "print(\"=\" * 60)\n",
    "dfs = clean_dataframes(dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "\n",
    "## Data Analysis Functions\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: ANALYZING DATA\n",
      "============================================================\n",
      "Found 5 dataframes in the environment\n",
      "\n",
      "==================================================\n",
      "DataFrame: df1 with shape (43, 6)\n",
      "==================================================\n",
      "\n",
      " Basic Information:\n",
      "  - Rows: 43\n",
      "  - Columns: 6\n",
      "  - Missing values: 46 (17.8% of all cells)\n",
      "\n",
      " Data Types:\n",
      "  - object: 6 columns\n",
      "\n",
      " Data Preview:\n",
      "      Stios Indicador  Enero Febrero  Marzo  Abril\n",
      "0      TSAN      Kw/h  54850   59992  48422  58710\n",
      "1       GLP       NaN  38136   41690  33650  40797\n",
      "2  AVIACION       NaN  15178    6500   8244   6612\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "DataFrame: df2 with shape (12, 9)\n",
      "==================================================\n",
      "\n",
      " Basic Information:\n",
      "  - Rows: 12\n",
      "  - Columns: 9\n",
      "  - Missing values: 3 (2.8% of all cells)\n",
      "\n",
      " Data Types:\n",
      "  - float64: 7 columns\n",
      "  - object: 2 columns\n",
      "\n",
      "锔 Found 5 unnamed columns\n",
      "\n",
      " Content Analysis:\n",
      "  - Potential sites: 3\n",
      "  - Potential indicators: 12\n",
      "\n",
      " Sample sites: 1.0, 3.0, 2.0\n",
      "\n",
      " Sample indicators: Combustibles - Diesel (Remolcadores y Barcazas), Combustibles - Diesel Flota puma , Combustible - Nafta vehiculos Flota Puma , Consumo de Agua terminales , Electricidad (Terminales)\n",
      "\n",
      " Data Preview:\n",
      "   Unnamed: 1                                       Unnamed: 2  \\\n",
      "0         1.0  Combustibles - Diesel (Remolcadores y Barcazas)   \n",
      "1         1.0                Combustibles - Diesel Flota puma    \n",
      "2         1.0        Combustible - Nafta vehiculos Flota Puma    \n",
      "\n",
      "                    Unnamed: 3     Enero   Febrero     Marzo     Abril  \\\n",
      "0  Consumo de Diesel en litros  617447.0  512860.0  635003.0  560917.0   \n",
      "1  Consumo de Diesel en litros    3094.0    2942.0    4216.0    3747.0   \n",
      "2  Consumo de Nafta en Litros      605.4     516.6     624.0     632.0   \n",
      "\n",
      "   Unnamed: 16  Unnamed: 17  \n",
      "0         2.67   6211.02609  \n",
      "1         2.67     37.37733  \n",
      "2         2.30      5.46940  \n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "DataFrame: df3 with shape (6, 7)\n",
      "==================================================\n",
      "\n",
      " Basic Information:\n",
      "  - Rows: 6\n",
      "  - Columns: 7\n",
      "  - Missing values: 1 (2.4% of all cells)\n",
      "\n",
      " Data Types:\n",
      "  - int64: 4 columns\n",
      "  - object: 2 columns\n",
      "  - float64: 1 columns\n",
      "\n",
      " Data Preview:\n",
      "  Material Reciclado  Enero  Febrero  Marzo  Abril  \\\n",
      "0       Papel/cart贸n     60       32     27     29   \n",
      "1           Pl谩stico      8        8     10      5   \n",
      "2           Aluminio      0        0      0      0   \n",
      "\n",
      "  Emisiones Evitadas (Aproximado)  Total  \n",
      "0   ~1,5 kg COe por kg reciclado  222.0  \n",
      "1   ~1,8 kg COe por kg reciclado   55.8  \n",
      "2   ~9,2 kg COe por kg reciclado    0.0  \n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "DataFrame: df4 with shape (5, 8)\n",
      "==================================================\n",
      "\n",
      " Basic Information:\n",
      "  - Rows: 5\n",
      "  - Columns: 8\n",
      "  - Missing values: 15 (37.5% of all cells)\n",
      "\n",
      " Data Types:\n",
      "  - object: 4 columns\n",
      "  - float64: 4 columns\n",
      "\n",
      " Data Preview:\n",
      "                                Indicador  \\\n",
      "1  Consumo de combustible Diesel (litros)   \n",
      "2   Reducci贸n mensual de Emisiones de CO2   \n",
      "3       Generaci贸n de residuos Reciclados   \n",
      "\n",
      "   Variaci贸n Porcentual Mensual Del Consumo Frecuencia   \\\n",
      "1  variaci贸n porcentual mensual del consumo     mensual   \n",
      "2                       Emisiones generadas     Mensual   \n",
      "3                  % de Residuos reciclados     Mensual   \n",
      "\n",
      "                                            Formula       Enero    Febrero  \\\n",
      "1  ((Consumo mes anterior - Consumo mes actual) /... -78.270093  20.305660   \n",
      "2  ((Emisiones del mes anterior - Emisiones mes a...        NaN  19.152501   \n",
      "3  (Total de residuos reciclados / total de resid...   1.151151   1.791312   \n",
      "\n",
      "       Marzo      Aril  \n",
      "1 -19.367385  0.117289  \n",
      "2 -22.025498  0.108906  \n",
      "3   1.369863  0.646326  \n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "DataFrame: df5 with shape (33, 8)\n",
      "==================================================\n",
      "\n",
      " Basic Information:\n",
      "  - Rows: 33\n",
      "  - Columns: 8\n",
      "  - Missing values: 158 (59.8% of all cells)\n",
      "\n",
      " Data Types:\n",
      "  - object: 7 columns\n",
      "  - float64: 1 columns\n",
      "\n",
      " Data Preview:\n",
      "   Iten       Origen                                           Objetivo  \\\n",
      "0   NaN          NaN                                                NaN   \n",
      "1   1.0  Master Plan  Reciclar toda la basura posible generada en nu...   \n",
      "2   NaN          NaN                                                NaN   \n",
      "\n",
      "                                                Meta  \\\n",
      "0                                                NaN   \n",
      "1  Lograr reciclar m铆nimo 391Kg de basura en las ...   \n",
      "2                                                NaN   \n",
      "\n",
      "                             Criterios Evaluaci贸n Responsable  \\\n",
      "0                                       Desempe帽o         NaN   \n",
      "1          Logra: Reciclar minimo 391kg de basura      Lorena   \n",
      "2  Supera: Reciclar entre 392kg a 500kg de basura         NaN   \n",
      "\n",
      "                                    Estrateg铆a \"Que\"  \\\n",
      "0                                                NaN   \n",
      "1  * Mejor segregaci贸n de residuos\\n*Mejor contro...   \n",
      "2                                                NaN   \n",
      "\n",
      "                                 Acciones Realizadas  \n",
      "0                                                NaN  \n",
      "1  Inspecciones y control para una correcta segre...  \n",
      "2  Procedimiento para la estandarizaci贸n de resid...  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataframes(dfs: dict) -> None:\n",
    "    \"\"\"Print comprehensive analysis of all DataFrames.\"\"\"\n",
    "    print(f\"Found {len(dfs)} dataframes in the environment\\n\")\n",
    "\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        _print_single_dataframe_info(df_name, df_obj)\n",
    "\n",
    "\n",
    "def _print_single_dataframe_info(df_name: str, df_obj: pd.DataFrame) -> None:\n",
    "    \"\"\"Print detailed information about a single DataFrame.\"\"\"\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"DataFrame: {df_name} with shape {df_obj.shape}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # Basic information\n",
    "    total_cells = df_obj.shape[0] * df_obj.shape[1]\n",
    "    missing_values = df_obj.isna().sum().sum()\n",
    "    missing_pct = (missing_values / total_cells) if total_cells > 0 else 0\n",
    "\n",
    "    print(\"\\n Basic Information:\")\n",
    "    print(f\"  - Rows: {df_obj.shape[0]}\")\n",
    "    print(f\"  - Columns: {df_obj.shape[1]}\")\n",
    "    print(\n",
    "        f\"  - Missing values: {missing_values} ({missing_pct:.1%} of all cells)\"\n",
    "    )\n",
    "\n",
    "    # Data types\n",
    "    print(\"\\n Data Types:\")\n",
    "    for dtype, count in df_obj.dtypes.value_counts().items():\n",
    "        print(f\"  - {dtype}: {count} columns\")\n",
    "\n",
    "    # Unnamed columns analysis\n",
    "    unnamed_cols = [col for col in df_obj.columns if \"Unnamed\" in str(col)]\n",
    "    if unnamed_cols:\n",
    "        print(f\"\\n锔 Found {len(unnamed_cols)} unnamed columns\")\n",
    "\n",
    "    # Content analysis for specific patterns\n",
    "    _analyze_content_patterns(df_obj)\n",
    "\n",
    "    # Preview\n",
    "    print(\"\\n Data Preview:\")\n",
    "    print(df_obj.head(3))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def _analyze_content_patterns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Analyze content patterns in DataFrame.\"\"\"\n",
    "    if \"Unnamed: 1\" in df.columns and \"Unnamed: 2\" in df.columns:\n",
    "        unique_sites = df[\"Unnamed: 1\"].dropna().unique()\n",
    "        unique_indicators = df[\"Unnamed: 2\"].dropna().unique()\n",
    "\n",
    "        print(\"\\n Content Analysis:\")\n",
    "        print(f\"  - Potential sites: {len(unique_sites)}\")\n",
    "        print(f\"  - Potential indicators: {len(unique_indicators)}\")\n",
    "\n",
    "        if len(unique_sites) > 0:\n",
    "            print(\"\\n Sample sites:\", \", \".join(map(str, unique_sites[:5])))\n",
    "        if len(unique_indicators) > 0:\n",
    "            print(\n",
    "                \"\\n Sample indicators:\",\n",
    "                \", \".join(map(str, unique_indicators[:5])),\n",
    "            )\n",
    "\n",
    "\n",
    "def preview_dataframes_html(dfs: dict) -> None:\n",
    "    \"\"\"Display DataFrames as scrollable HTML tables.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"{'=' * 50}\")\n",
    "        print(f\"Previewing {df_name}...\")\n",
    "\n",
    "        html_table = df_obj.head(10).to_html(index=False, max_rows=10)\n",
    "        styled_html = f\"\"\"\n",
    "        <div style=\"overflow-x: auto; max-height: 500px; overflow-y: auto;\">\n",
    "            {html_table}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(styled_html))\n",
    "        print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "\n",
    "def display_datatypes(dfs: dict) -> None:\n",
    "    \"\"\"Display data types for all DataFrames in HTML tables.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"{'=' * 50}\")\n",
    "        print(f\"DataFrame: {df_name} with shape {df_obj.shape}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "\n",
    "        dtype_df = pd.DataFrame(df_obj.dtypes).reset_index()\n",
    "        dtype_df.columns = [\"Column\", \"Data Type\"]\n",
    "        display(HTML(dtype_df.to_html(index=False)))\n",
    "        print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "\n",
    "# Step 3: Analyze data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: ANALYZING DATA\")\n",
    "print(\"=\" * 60)\n",
    "analyze_dataframes(dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "\n",
    "## Specialized Data Processing Functions\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: PROCESSING MAIN DATAFRAME\n",
      "============================================================\n",
      "Replaced zeros and NaNs in Enero with mean: 193360.00\n",
      "Replaced zeros and NaNs in Febrero with mean: 960.00\n",
      "Replaced zeros and NaNs in Marzo with mean: 740.00\n",
      "Replaced zeros and NaNs in Abril with mean: 630.00\n",
      "Replaced zeros and NaNs in Enero with mean: 14820.00\n",
      "Replaced zeros and NaNs in Febrero with mean: 23023.33\n",
      "Replaced zeros and NaNs in Marzo with mean: 29185.00\n",
      "Replaced zeros and NaNs in Abril with mean: 16935.00\n",
      "Replaced zeros and NaNs in Enero with mean: 1498.50\n",
      "Replaced zeros and NaNs in Febrero with mean: 558.25\n",
      "Replaced zeros and NaNs in Marzo with mean: 675.25\n",
      "Replaced zeros and NaNs in Abril with mean: 1052.10\n",
      "Converted gallons to liters and renamed indicator\n",
      "Added Total column to Section_0\n",
      "Added Total column to Section_1\n",
      "Added Total column to Section_2\n",
      "Added Total column to Section_3\n",
      "Added Total column to Section_4\n",
      "Added Total column to Section_5\n",
      "Added Total column to Section_6\n",
      "\n",
      "============================================================\n",
      "STEP 6: SAVING PROCESSED DATA\n",
      "============================================================\n",
      "Saved section_0 to ../../data/processed/sites/df1/section_0.csv\n",
      "Saved section_1 to ../../data/processed/sites/df1/section_1.csv\n",
      "Saved section_2 to ../../data/processed/sites/df1/section_2.csv\n",
      "Saved section_3 to ../../data/processed/sites/df1/section_3.csv\n",
      "Saved section_4 to ../../data/processed/sites/df1/section_4.csv\n",
      "Saved section_5 to ../../data/processed/sites/df1/section_5.csv\n",
      "Saved section_6 to ../../data/processed/sites/df1/section_6.csv\n"
     ]
    }
   ],
   "source": [
    "def process_main_dataframe(dfs: dict, df_key: str = \"df1\") -> dict:\n",
    "    \"\"\"\n",
    "    Process the main DataFrame with specialized business logic.\n",
    "\n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "        df_key (str): Key of the main DataFrame to process\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of processed section DataFrames\n",
    "    \"\"\"\n",
    "    if df_key not in dfs:\n",
    "        raise ValueError(f\"DataFrame '{df_key}' not found in dfs\")\n",
    "\n",
    "    df = dfs[df_key].copy()\n",
    "\n",
    "    # Remove total rows and reset index\n",
    "    df = df[~df[\"Stios\"].isin([\"TOTAL PY\"])].reset_index(drop=True)\n",
    "\n",
    "    # Split into sections\n",
    "    site_dfs = _split_dataframe_by_sections(df)\n",
    "\n",
    "    # Clean sections\n",
    "    site_dfs = _clean_section_data(site_dfs)\n",
    "\n",
    "    # Apply specialized processing\n",
    "    site_dfs = _apply_specialized_processing(site_dfs)\n",
    "\n",
    "    # Add calculated columns\n",
    "    site_dfs = _add_calculated_columns(site_dfs)\n",
    "\n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _split_dataframe_by_sections(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Split DataFrame into sections using TOTAL rows as markers.\"\"\"\n",
    "    site_dfs = {}\n",
    "    current_section = 0\n",
    "    current_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if \"TOTAL\" in str(row[\"Stios\"]):\n",
    "            # Save previous section\n",
    "            if current_rows:\n",
    "                site_dfs[f\"Section_{current_section}\"] = pd.DataFrame(\n",
    "                    current_rows\n",
    "                ).reset_index(drop=True)\n",
    "                current_section += 1\n",
    "            current_rows = [row]\n",
    "        else:\n",
    "            current_rows.append(row)\n",
    "\n",
    "    # Save last section\n",
    "    if current_rows:\n",
    "        site_dfs[f\"Section_{current_section}\"] = pd.DataFrame(\n",
    "            current_rows\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Handle Section_5 split\n",
    "    if \"Section_5\" in site_dfs and len(site_dfs[\"Section_5\"]) > 2:\n",
    "        last_2_rows = site_dfs[\"Section_5\"].tail(2).copy()\n",
    "        site_dfs[\"Section_5\"] = (\n",
    "            site_dfs[\"Section_5\"].iloc[:-2].reset_index(drop=True)\n",
    "        )\n",
    "        site_dfs[\"Section_6\"] = last_2_rows.reset_index(drop=True)\n",
    "\n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _clean_section_data(site_dfs: dict) -> dict:\n",
    "    \"\"\"Clean section data by removing header rows and filling NaN values.\"\"\"\n",
    "    for i in range(1, 6):\n",
    "        section_name = f\"Section_{i}\"\n",
    "        if section_name in site_dfs and len(site_dfs[section_name]) > 1:\n",
    "            # Remove first two rows (headers)\n",
    "            site_dfs[section_name] = (\n",
    "                site_dfs[section_name].iloc[2:].reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Fill NaN values in Indicador column\n",
    "            if \"Indicador\" in site_dfs[section_name].columns:\n",
    "                df = site_dfs[section_name]\n",
    "                valid_values = df[\"Indicador\"].dropna()\n",
    "                valid_values = valid_values[\n",
    "                    ~valid_values.str.lower().eq(\"indicador\")\n",
    "                ]\n",
    "\n",
    "                if not valid_values.empty:\n",
    "                    first_valid = valid_values.iloc[0]\n",
    "                    df[\"Indicador\"] = df[\"Indicador\"].fillna(first_valid)\n",
    "\n",
    "    # Remove last row from Section_5\n",
    "    if \"Section_5\" in site_dfs and len(site_dfs[\"Section_5\"]) > 0:\n",
    "        site_dfs[\"Section_5\"] = (\n",
    "            site_dfs[\"Section_5\"].iloc[:-1].reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _apply_specialized_processing(site_dfs: dict) -> dict:\n",
    "    \"\"\"Apply specialized processing to specific sections.\"\"\"\n",
    "    # Process Section_1: Replace NaNs with column means (df2 processing)\n",
    "    if \"Section_1\" in site_dfs:\n",
    "        site_dfs[\"Section_1\"] = _replace_zeros_with_means(\n",
    "            site_dfs[\"Section_1\"]\n",
    "        )\n",
    "\n",
    "    # Process Section_2: Replace zeros with column means\n",
    "    if \"Section_2\" in site_dfs:\n",
    "        site_dfs[\"Section_2\"] = _replace_zeros_with_means(\n",
    "            site_dfs[\"Section_2\"]\n",
    "        )\n",
    "\n",
    "    # Process Section_3: Replace missing values with column means\n",
    "    if \"Section_3\" in site_dfs:\n",
    "        site_dfs[\"Section_3\"] = _replace_zeros_with_means(\n",
    "            site_dfs[\"Section_3\"]\n",
    "        )\n",
    "\n",
    "    # Process Section_6: Convert gallons to liters\n",
    "    if \"Section_6\" in site_dfs:\n",
    "        site_dfs[\"Section_6\"] = _convert_gallons_to_liters(\n",
    "            site_dfs[\"Section_6\"]\n",
    "        )\n",
    "\n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _replace_zeros_with_means(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replace zero values with column means for numeric columns.\"\"\"\n",
    "    month_columns = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "\n",
    "    for col in month_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "            # Calculate mean of non-zero values\n",
    "            non_zero_values = df[col][(df[col] != 0) & (df[col].notna())]\n",
    "\n",
    "            if len(non_zero_values) > 0:\n",
    "                col_mean = non_zero_values.mean()\n",
    "\n",
    "                # Replace zeros and NaNs with mean\n",
    "                df[col] = df[col].replace(0, col_mean).fillna(col_mean)\n",
    "                print(\n",
    "                    f\"Replaced zeros and NaNs in {col} with mean: {col_mean:.2f}\"\n",
    "                )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _convert_gallons_to_liters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert gallons to liters in the first row if applicable.\"\"\"\n",
    "    if len(df) > 0 and \"Galones\" in str(df.iloc[0][\"Indicador\"]):\n",
    "        month_cols = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "        for col in month_cols:\n",
    "            if col in df.columns:\n",
    "                df.loc[0, col] = df.iloc[0][col] * 3.78541\n",
    "\n",
    "        df.loc[0, \"Indicador\"] = \"Consumo de Diesel (Litros)\"\n",
    "        print(\"Converted gallons to liters and renamed indicator\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _add_calculated_columns(site_dfs: dict) -> dict:\n",
    "    \"\"\"Add calculated Total column to all sections.\"\"\"\n",
    "    numeric_columns = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "\n",
    "    for section_name, df in site_dfs.items():\n",
    "        if \"Total\" not in df.columns:\n",
    "            # Convert to numeric and calculate totals\n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "            df[\"Total\"] = df[numeric_columns].sum(axis=1, skipna=True)\n",
    "            site_dfs[section_name] = df\n",
    "            print(f\"Added Total column to {section_name}\")\n",
    "\n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def save_sections_to_csv(site_dfs: dict, output_dir: str) -> None:\n",
    "    \"\"\"Save each section to a separate CSV file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for section_name, df in site_dfs.items():\n",
    "        filename = section_name.replace(\"Section_\", \"section_\").lower()\n",
    "        filepath = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {filename} to {filepath}\")\n",
    "\n",
    "\n",
    "# Step 5: Process main DataFrame\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: PROCESSING MAIN DATAFRAME\")\n",
    "print(\"=\" * 60)\n",
    "site_dfs = process_main_dataframe(dfs)\n",
    "\n",
    "# Step 6: Save processed data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "save_sections_to_csv(site_dfs, OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
