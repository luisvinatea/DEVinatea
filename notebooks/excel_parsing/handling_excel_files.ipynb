{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Excel Files in Python\n",
    "\n",
    "**Author:** Luis Paulo Vinatea Barberena  \n",
    "**Date:** 2025-05-21  \n",
    "\n",
    "When you need to work with Excel files in Python, you can use the `openpyxl` library. This library allows you to read and write Excel files in the `.xlsx` format.\n",
    "It is a powerful tool for data manipulation and analysis, especially when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import standard libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import openpyxl\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "import hashlib\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Import project utilities\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from src.utils.data_processing import *  # noqa: F403\n",
    "from src.utils.visualization import *  # noqa: F403\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "XLSX_DATA_PATH = \"../../data/raw/*.xlsx\"\n",
    "OUTPUT_DIR = \"../../data/processed/sites/df1/\"\n",
    "\n",
    "# Global visualization settings\n",
    "used_colors = set()\n",
    "site_color_mapping = {}\n",
    "available_css4_colors = list(mcolors.CSS4_COLORS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_excel_files(data_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load all Excel files from the specified path and create DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path pattern for Excel files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing loaded DataFrames\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    df_count = 1\n",
    "    \n",
    "    try:\n",
    "        for file in glob.glob(data_path):\n",
    "            print(f\"Loading data from {file}\")\n",
    "            \n",
    "            # Load workbook with openpyxl for inspection\n",
    "            wb = openpyxl.load_workbook(file, data_only=True, read_only=True)\n",
    "            file_basename = os.path.basename(file)\n",
    "            \n",
    "            print(f\"\\nWorkbook: {file_basename}\")\n",
    "            print(f\"Contains {len(wb.sheetnames)} sheets: {', '.join(wb.sheetnames)}\")\n",
    "            \n",
    "            # Process each sheet\n",
    "            for sheet_name in wb.sheetnames:\n",
    "                df_name = f\"df{df_count}\"\n",
    "                df = _load_sheet_with_header_detection(file, sheet_name, wb[sheet_name])\n",
    "                \n",
    "                if df is not None:\n",
    "                    dfs[df_name] = df\n",
    "                    print(f\"  Loaded {df_name} with shape {df.shape}\")\n",
    "                    df_count += 1\n",
    "            \n",
    "            wb.close()\n",
    "            \n",
    "        print(f\"\\nSuccessfully loaded {df_count - 1} dataframes.\")\n",
    "        return dfs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def _load_sheet_with_header_detection(file_path: str, sheet_name: str, sheet) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a sheet with automatic header detection.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "        sheet_name (str): Name of the sheet\n",
    "        sheet: Openpyxl sheet object\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame or None if sheet is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if sheet is empty\n",
    "        if not sheet.max_row or not sheet.max_column:\n",
    "            print(f\"  Warning: Sheet '{sheet_name}' appears to be empty or corrupted\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n  Sheet: '{sheet_name}'\")\n",
    "        print(f\"  Dimensions: {sheet.max_row} rows x {sheet.max_column} columns\")\n",
    "        \n",
    "        # Check for merged cells\n",
    "        try:\n",
    "            merged_cells = list(sheet.merged_cells.ranges)\n",
    "            if merged_cells:\n",
    "                print(f\"  Contains {len(merged_cells)} merged cell ranges\")\n",
    "        except AttributeError:\n",
    "            print(\"  Note: Cannot check merged cells in read-only mode\")\n",
    "        \n",
    "        # Load initial DataFrame without headers\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            na_values=[\"NA\", \"N/A\", \"\"],\n",
    "            keep_default_na=True,\n",
    "        )\n",
    "        \n",
    "        # Detect header row\n",
    "        header_row = _detect_header_row(df)\n",
    "        \n",
    "        if header_row is not None:\n",
    "            # Reload with detected header\n",
    "            df = pd.read_excel(\n",
    "                file_path,\n",
    "                sheet_name=sheet_name,\n",
    "                header=header_row,\n",
    "                na_values=[\"NA\", \"N/A\", \"\"],\n",
    "                keep_default_na=True,\n",
    "            )\n",
    "            print(f\"  Detected header at row {header_row + 1}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading sheet '{sheet_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _detect_header_row(df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of the header row, or None if not found\n",
    "    \"\"\"\n",
    "    for i in range(min(10, len(df))):\n",
    "        str_count = sum(1 for x in df.iloc[i] if isinstance(x, str))\n",
    "        if str_count > 0.5 * df.shape[1]:  # More than half are strings\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def clean_dataframes(dfs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Apply comprehensive cleaning to all DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning process...\")\n",
    "    \n",
    "    # Apply cleaning steps in order\n",
    "    dfs = standardize_headers(dfs)\n",
    "    dfs = remove_empty_rows_and_cols(dfs)\n",
    "    dfs = handle_nan_values(dfs)\n",
    "    dfs = title_case_columns(dfs)\n",
    "    \n",
    "    print(\"Data cleaning completed!\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def standardize_headers(dfs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Standardize column headers across all DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with standardized headers\n",
    "    \"\"\"\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(\"STANDARDIZING DATAFRAME HEADERS\")\n",
    "    print(f\"{'=' * 50}\\n\")\n",
    "    \n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"Processing {df_name}...\")\n",
    "        \n",
    "        # Handle unnamed columns\n",
    "        df_obj = _fix_unnamed_columns(df_obj)\n",
    "        \n",
    "        # Standardize date columns\n",
    "        df_obj = _standardize_date_columns(df_obj)\n",
    "        \n",
    "        dfs[df_name] = df_obj\n",
    "    \n",
    "    print(\"\\nHeader standardization complete!\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def _fix_unnamed_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fix unnamed columns by finding header values in data rows.\"\"\"\n",
    "    unnamed_cols = [col for col in df.columns if \"Unnamed\" in str(col)]\n",
    "    \n",
    "    if len(unnamed_cols) > 0:\n",
    "        print(f\"  Found {len(unnamed_cols)} unnamed columns\")\n",
    "        \n",
    "        for i in range(min(5, len(df))):\n",
    "            row = df.iloc[i]\n",
    "            str_values = [v for v in row if isinstance(v, str) and pd.notna(v)]\n",
    "            \n",
    "            if len(str_values) >= len(unnamed_cols) * 0.7:\n",
    "                new_names = {}\n",
    "                original_cols = df.columns.tolist()\n",
    "                \n",
    "                for j, col in enumerate(original_cols):\n",
    "                    if (\"Unnamed\" in str(col) and j < len(row) and \n",
    "                        pd.notna(row.iloc[j]) and isinstance(row.iloc[j], str)):\n",
    "                        new_names[col] = str(row.iloc[j]).strip()\n",
    "                \n",
    "                if new_names:\n",
    "                    print(f\"  Found potential header values in row {i + 1}\")\n",
    "                    print(f\"  Renaming {len(new_names)} columns\")\n",
    "                    \n",
    "                    df = df.rename(columns=new_names)\n",
    "                    df = df.drop(index=i)\n",
    "                    print(f\"  New columns: {list(new_names.values())}\")\n",
    "                    break\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _standardize_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize date column names to Spanish format.\"\"\"\n",
    "    date_mappings = {\n",
    "        \"January\": \"Enero\", \"February\": \"Febrero\", \"March\": \"Marzo\",\n",
    "        \"April\": \"Abril\", \"May\": \"Mayo\", \"June\": \"Junio\",\n",
    "        \"July\": \"Julio\", \"August\": \"Agosto\", \"September\": \"Septiembre\",\n",
    "        \"October\": \"Octubre\", \"November\": \"Noviembre\", \"December\": \"Diciembre\"\n",
    "    }\n",
    "    \n",
    "    date_cols = [col for col in df.columns if str(col) in date_mappings]\n",
    "    if date_cols:\n",
    "        print(f\"  Standardizing {len(date_cols)} date-related columns to Spanish format\")\n",
    "        df = df.rename(columns=date_mappings)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_empty_rows_and_cols(dfs: dict) -> dict:\n",
    "    \"\"\"Remove completely empty rows and columns from all DataFrames.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        original_shape = df_obj.shape\n",
    "        df_obj = df_obj.dropna(how='all').dropna(axis=1, how='all')\n",
    "        dfs[df_name] = df_obj\n",
    "        print(f\"Cleaned {df_name}: {original_shape} -> {df_obj.shape}\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def handle_nan_values(dfs: dict) -> dict:\n",
    "    \"\"\"Analyze and selectively handle NaN values in DataFrames.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"\\n{'-' * 50}\")\n",
    "        print(f\"NaN analysis for {df_name}:\")\n",
    "        \n",
    "        # Calculate NaN statistics\n",
    "        total_cells = df_obj.shape[0] * df_obj.shape[1]\n",
    "        nan_count = df_obj.isna().sum().sum()\n",
    "        nan_percentage = (nan_count / total_cells) if total_cells > 0 else 0\n",
    "        \n",
    "        print(f\"- Total NaN values: {nan_count} ({nan_percentage:.1%} of all cells)\")\n",
    "        \n",
    "        # Handle high-NaN columns\n",
    "        df_obj = _remove_high_nan_columns(df_obj)\n",
    "        \n",
    "        # Handle critical row removal\n",
    "        df_obj = _remove_critical_nan_rows(df_obj)\n",
    "        \n",
    "        dfs[df_name] = df_obj\n",
    "        print(f\"- Final shape after analysis: {df_obj.shape}\")\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "def _remove_high_nan_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove columns with >80% NaN values.\"\"\"\n",
    "    column_nan_pct = df.isna().mean().sort_values(ascending=False)\n",
    "    high_nan_cols = column_nan_pct[column_nan_pct > 0.8].index.tolist()\n",
    "    \n",
    "    if high_nan_cols:\n",
    "        print(f\"- Columns with >80% NaNs: {', '.join(high_nan_cols)}\")\n",
    "        df = df.drop(columns=high_nan_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _remove_critical_nan_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows where critical columns are all NaN.\"\"\"\n",
    "    critical_cols = [col for col in df.columns if \"Unnamed\" not in str(col)]\n",
    "    \n",
    "    if not critical_cols and len(df.columns) > 2:\n",
    "        critical_cols = [df.columns[1], df.columns[2]]\n",
    "    \n",
    "    if critical_cols:\n",
    "        print(f\"- Critical columns: {', '.join(map(str, critical_cols))}\")\n",
    "        rows_to_drop = df[df[critical_cols].isna().all(axis=1)].index\n",
    "        print(f\"- Found {len(rows_to_drop)} rows where all critical columns are NaN\")\n",
    "        df = df.drop(index=rows_to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def title_case_columns(dfs: dict) -> dict:\n",
    "    \"\"\"Apply title case to all column names.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        df_obj.columns = df_obj.columns.str.title()\n",
    "        dfs[df_name] = df_obj\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA ANALYSIS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_dataframes(dfs: dict) -> None:\n",
    "    \"\"\"Print comprehensive analysis of all DataFrames.\"\"\"\n",
    "    print(f\"Found {len(dfs)} dataframes in the environment\\n\")\n",
    "    \n",
    "    for df_name, df_obj in dfs.items():\n",
    "        _print_single_dataframe_info(df_name, df_obj)\n",
    "\n",
    "\n",
    "def _print_single_dataframe_info(df_name: str, df_obj: pd.DataFrame) -> None:\n",
    "    \"\"\"Print detailed information about a single DataFrame.\"\"\"\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"DataFrame: {df_name} with shape {df_obj.shape}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Basic information\n",
    "    total_cells = df_obj.shape[0] * df_obj.shape[1]\n",
    "    missing_values = df_obj.isna().sum().sum()\n",
    "    missing_pct = (missing_values / total_cells) if total_cells > 0 else 0\n",
    "    \n",
    "    print(\"\\nðŸ“Š Basic Information:\")\n",
    "    print(f\"  - Rows: {df_obj.shape[0]}\")\n",
    "    print(f\"  - Columns: {df_obj.shape[1]}\")\n",
    "    print(f\"  - Missing values: {missing_values} ({missing_pct:.1%} of all cells)\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nðŸ“‹ Data Types:\")\n",
    "    for dtype, count in df_obj.dtypes.value_counts().items():\n",
    "        print(f\"  - {dtype}: {count} columns\")\n",
    "    \n",
    "    # Unnamed columns analysis\n",
    "    unnamed_cols = [col for col in df_obj.columns if \"Unnamed\" in str(col)]\n",
    "    if unnamed_cols:\n",
    "        print(f\"\\nâš ï¸ Found {len(unnamed_cols)} unnamed columns\")\n",
    "    \n",
    "    # Content analysis for specific patterns\n",
    "    _analyze_content_patterns(df_obj)\n",
    "    \n",
    "    # Preview\n",
    "    print(\"\\nðŸ” Data Preview:\")\n",
    "    print(df_obj.head(3))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def _analyze_content_patterns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Analyze content patterns in DataFrame.\"\"\"\n",
    "    if \"Unnamed: 1\" in df.columns and \"Unnamed: 2\" in df.columns:\n",
    "        unique_sites = df[\"Unnamed: 1\"].dropna().unique()\n",
    "        unique_indicators = df[\"Unnamed: 2\"].dropna().unique()\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ Content Analysis:\")\n",
    "        print(f\"  - Potential sites: {len(unique_sites)}\")\n",
    "        print(f\"  - Potential indicators: {len(unique_indicators)}\")\n",
    "        \n",
    "        if len(unique_sites) > 0:\n",
    "            print(\"\\nðŸ¢ Sample sites:\", \", \".join(map(str, unique_sites[:5])))\n",
    "        if len(unique_indicators) > 0:\n",
    "            print(\"\\nðŸ“ Sample indicators:\", \", \".join(map(str, unique_indicators[:5])))\n",
    "\n",
    "\n",
    "def preview_dataframes_html(dfs: dict) -> None:\n",
    "    \"\"\"Display DataFrames as scrollable HTML tables.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"{'=' * 50}\")\n",
    "        print(f\"Previewing {df_name}...\")\n",
    "        \n",
    "        html_table = df_obj.head(10).to_html(index=False, max_rows=10)\n",
    "        styled_html = f\"\"\"\n",
    "        <div style=\"overflow-x: auto; max-height: 500px; overflow-y: auto;\">\n",
    "            {html_table}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(styled_html))\n",
    "        print(f\"{'=' * 50}\\n\")\n",
    "\n",
    "\n",
    "def display_datatypes(dfs: dict) -> None:\n",
    "    \"\"\"Display data types for all DataFrames in HTML tables.\"\"\"\n",
    "    for df_name, df_obj in dfs.items():\n",
    "        print(f\"{'=' * 50}\")\n",
    "        print(f\"DataFrame: {df_name} with shape {df_obj.shape}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "        \n",
    "        dtype_df = pd.DataFrame(df_obj.dtypes).reset_index()\n",
    "        dtype_df.columns = [\"Column\", \"Data Type\"]\n",
    "        display(HTML(dtype_df.to_html(index=False)))\n",
    "        print(f\"{'=' * 50}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPECIALIZED DATA PROCESSING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_main_dataframe(dfs: dict, df_key: str = \"df1\") -> dict:\n",
    "    \"\"\"\n",
    "    Process the main DataFrame with specialized business logic.\n",
    "    \n",
    "    Args:\n",
    "        dfs (dict): Dictionary of DataFrames\n",
    "        df_key (str): Key of the main DataFrame to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of processed section DataFrames\n",
    "    \"\"\"\n",
    "    if df_key not in dfs:\n",
    "        raise ValueError(f\"DataFrame '{df_key}' not found in dfs\")\n",
    "    \n",
    "    df = dfs[df_key].copy()\n",
    "    \n",
    "    # Remove total rows and reset index\n",
    "    df = df[~df[\"Stios\"].isin([\"TOTAL PY\"])].reset_index(drop=True)\n",
    "    \n",
    "    # Split into sections\n",
    "    site_dfs = _split_dataframe_by_sections(df)\n",
    "    \n",
    "    # Clean sections\n",
    "    site_dfs = _clean_section_data(site_dfs)\n",
    "    \n",
    "    # Apply specialized processing\n",
    "    site_dfs = _apply_specialized_processing(site_dfs)\n",
    "    \n",
    "    # Add calculated columns\n",
    "    site_dfs = _add_calculated_columns(site_dfs)\n",
    "    \n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _split_dataframe_by_sections(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Split DataFrame into sections using TOTAL rows as markers.\"\"\"\n",
    "    site_dfs = {}\n",
    "    current_section = 0\n",
    "    current_rows = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if \"TOTAL\" in str(row[\"Stios\"]):\n",
    "            # Save previous section\n",
    "            if current_rows:\n",
    "                site_dfs[f\"Section_{current_section}\"] = pd.DataFrame(current_rows).reset_index(drop=True)\n",
    "                current_section += 1\n",
    "            current_rows = [row]\n",
    "        else:\n",
    "            current_rows.append(row)\n",
    "    \n",
    "    # Save last section\n",
    "    if current_rows:\n",
    "        site_dfs[f\"Section_{current_section}\"] = pd.DataFrame(current_rows).reset_index(drop=True)\n",
    "    \n",
    "    # Handle Section_5 split\n",
    "    if \"Section_5\" in site_dfs and len(site_dfs[\"Section_5\"]) > 2:\n",
    "        last_2_rows = site_dfs[\"Section_5\"].tail(2).copy()\n",
    "        site_dfs[\"Section_5\"] = site_dfs[\"Section_5\"].iloc[:-2].reset_index(drop=True)\n",
    "        site_dfs[\"Section_6\"] = last_2_rows.reset_index(drop=True)\n",
    "    \n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _clean_section_data(site_dfs: dict) -> dict:\n",
    "    \"\"\"Clean section data by removing header rows and filling NaN values.\"\"\"\n",
    "    for i in range(1, 6):\n",
    "        section_name = f\"Section_{i}\"\n",
    "        if section_name in site_dfs and len(site_dfs[section_name]) > 1:\n",
    "            # Remove first two rows (headers)\n",
    "            site_dfs[section_name] = site_dfs[section_name].iloc[2:].reset_index(drop=True)\n",
    "            \n",
    "            # Fill NaN values in Indicador column\n",
    "            if \"Indicador\" in site_dfs[section_name].columns:\n",
    "                df = site_dfs[section_name]\n",
    "                valid_values = df[\"Indicador\"].dropna()\n",
    "                valid_values = valid_values[~valid_values.str.lower().eq(\"indicador\")]\n",
    "                \n",
    "                if not valid_values.empty:\n",
    "                    first_valid = valid_values.iloc[0]\n",
    "                    df[\"Indicador\"] = df[\"Indicador\"].fillna(first_valid)\n",
    "    \n",
    "    # Remove last row from Section_5\n",
    "    if \"Section_5\" in site_dfs and len(site_dfs[\"Section_5\"]) > 0:\n",
    "        site_dfs[\"Section_5\"] = site_dfs[\"Section_5\"].iloc[:-1].reset_index(drop=True)\n",
    "    \n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _apply_specialized_processing(site_dfs: dict) -> dict:\n",
    "    \"\"\"Apply specialized processing to specific sections.\"\"\"\n",
    "    # Process Section_2: Replace zeros with column means\n",
    "    if \"Section_2\" in site_dfs:\n",
    "        site_dfs[\"Section_2\"] = _replace_zeros_with_means(site_dfs[\"Section_2\"])\n",
    "    \n",
    "    # Process Section_6: Convert gallons to liters\n",
    "    if \"Section_6\" in site_dfs:\n",
    "        site_dfs[\"Section_6\"] = _convert_gallons_to_liters(site_dfs[\"Section_6\"])\n",
    "    \n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def _replace_zeros_with_means(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replace zero values with column means for numeric columns.\"\"\"\n",
    "    month_columns = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "    \n",
    "    for col in month_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Calculate mean of non-zero values\n",
    "            non_zero_values = df[col][(df[col] != 0) & (df[col].notna())]\n",
    "            \n",
    "            if len(non_zero_values) > 0:\n",
    "                col_mean = non_zero_values.mean()\n",
    "                \n",
    "                # Replace zeros and NaNs with mean\n",
    "                df[col] = df[col].replace(0, col_mean).fillna(col_mean)\n",
    "                print(f\"Replaced zeros and NaNs in {col} with mean: {col_mean:.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _convert_gallons_to_liters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert gallons to liters in the first row if applicable.\"\"\"\n",
    "    if len(df) > 0 and \"Galones\" in str(df.iloc[0][\"Indicador\"]):\n",
    "        month_cols = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "        for col in month_cols:\n",
    "            if col in df.columns:\n",
    "                df.loc[0, col] = df.iloc[0][col] * 3.78541\n",
    "        \n",
    "        df.loc[0, \"Indicador\"] = \"Consumo de Diesel (Litros)\"\n",
    "        print(\"Converted gallons to liters and renamed indicator\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def _add_calculated_columns(site_dfs: dict) -> dict:\n",
    "    \"\"\"Add calculated Total column to all sections.\"\"\"\n",
    "    numeric_columns = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "    \n",
    "    for section_name, df in site_dfs.items():\n",
    "        if \"Total\" not in df.columns:\n",
    "            # Convert to numeric and calculate totals\n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            df[\"Total\"] = df[numeric_columns].sum(axis=1, skipna=True)\n",
    "            site_dfs[section_name] = df\n",
    "            print(f\"Added Total column to {section_name}\")\n",
    "    \n",
    "    return site_dfs\n",
    "\n",
    "\n",
    "def save_sections_to_csv(site_dfs: dict, output_dir: str) -> None:\n",
    "    \"\"\"Save each section to a separate CSV file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for section_name, df in site_dfs.items():\n",
    "        filename = section_name.replace(\"Section_\", \"section_\").lower()\n",
    "        filepath = os.path.join(output_dir, f\"{filename}.csv\")\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {filename} to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def get_site_color(site_name: str) -> str:\n",
    "    \"\"\"Get a consistent unique color for a specific site.\"\"\"\n",
    "    global used_colors, site_color_mapping\n",
    "    \n",
    "    if site_name in site_color_mapping:\n",
    "        return site_color_mapping[site_name]\n",
    "    \n",
    "    unused_colors = [\n",
    "        color_name for color_name in available_css4_colors\n",
    "        if mcolors.CSS4_COLORS[color_name] not in used_colors\n",
    "    ]\n",
    "    \n",
    "    if unused_colors:\n",
    "        random.seed(hash(site_name))\n",
    "        selected_color_name = random.choice(unused_colors)\n",
    "        selected_color_hex = mcolors.CSS4_COLORS[selected_color_name]\n",
    "        used_colors.add(selected_color_hex)\n",
    "        site_color_mapping[site_name] = selected_color_hex\n",
    "        return selected_color_hex\n",
    "    \n",
    "    # Fallback for when all colors are used\n",
    "    hash_value = int(hashlib.md5(site_name.encode()).hexdigest()[:8], 16)\n",
    "    color_index = hash_value % len(available_css4_colors)\n",
    "    fallback_color_name = available_css4_colors[color_index]\n",
    "    fallback_color_hex = mcolors.CSS4_COLORS[fallback_color_name]\n",
    "    site_color_mapping[site_name] = fallback_color_hex\n",
    "    return fallback_color_hex\n",
    "\n",
    "\n",
    "def create_environmental_indicators_chart(site_dfs: dict) -> None:\n",
    "    \"\"\"Create comprehensive environmental indicators visualization.\"\"\"\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    num_sections = len(site_dfs)\n",
    "    cols = 2\n",
    "    rows = math.ceil(num_sections / cols)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 8 * rows))\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    fig.suptitle(\"Indicadores Ambientales por Sitio\", fontsize=20, fontweight=\"bold\", \n",
    "                y=0.98, color=\"#2C3E50\")\n",
    "    \n",
    "    # Prepare axes\n",
    "    if num_sections == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Track all sites for global legend\n",
    "    all_sites = set()\n",
    "    \n",
    "    # Create charts for each section\n",
    "    for idx, (section_name, df) in enumerate(site_dfs.items()):\n",
    "        ax = axes[idx]\n",
    "        _create_single_section_chart(ax, df, all_sites)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_sections, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    # Add global legend\n",
    "    _add_global_legend(fig, all_sites)\n",
    "    \n",
    "    # Final layout adjustments\n",
    "    plt.tight_layout(pad=3.0, rect=[0, 0, 0.92, 0.96])\n",
    "    fig.patch.set_edgecolor(\"#BDC3C7\")\n",
    "    fig.patch.set_linewidth(2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _create_single_section_chart(ax, df: pd.DataFrame, all_sites: set) -> None:\n",
    "    \"\"\"Create chart for a single section.\"\"\"\n",
    "    month_cols = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"]\n",
    "    plot_data = df[df[month_cols].notna().any(axis=1)].copy()\n",
    "    \n",
    "    if len(plot_data) > 0:\n",
    "        plot_data = plot_data.set_index(\"Stios\")[month_cols]\n",
    "        site_names = plot_data.index.tolist()\n",
    "        colors = [get_site_color(site) for site in site_names]\n",
    "        all_sites.update(site_names)\n",
    "        \n",
    "        # Create stacked bar chart\n",
    "        plot_data.T.plot(kind=\"bar\", stacked=True, ax=ax, color=colors, alpha=0.85,\n",
    "                        width=0.7, edgecolor=\"white\", linewidth=1.5, legend=False)\n",
    "        \n",
    "        # Get title from first Indicador value\n",
    "        title = _get_section_title(df)\n",
    "        ax.set_title(title, fontweight=\"bold\", fontsize=14, pad=15, \n",
    "                    color=\"#2C3E50\", fontfamily=\"serif\")\n",
    "        \n",
    "        # Style the chart\n",
    "        _style_chart(ax)\n",
    "    else:\n",
    "        _create_no_data_chart(ax, \"No hay datos disponibles\")\n",
    "\n",
    "\n",
    "def _get_section_title(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Get appropriate title for section chart.\"\"\"\n",
    "    if \"Indicador\" in df.columns:\n",
    "        indicador_values = df[\"Indicador\"].dropna()\n",
    "        if len(indicador_values) > 0:\n",
    "            return indicador_values.iloc[0]\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def _style_chart(ax) -> None:\n",
    "    \"\"\"Apply consistent styling to chart.\"\"\"\n",
    "    # Remove labels and enhance styling\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "    # Style ticks\n",
    "    ax.tick_params(axis=\"x\", rotation=0, labelsize=9, colors=\"#2C3E50\", length=0)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, colors=\"#34495E\")\n",
    "    \n",
    "    # Style grid and spines\n",
    "    ax.grid(True, alpha=0.4, linestyle=\"--\", linewidth=0.8, color=\"#BDC3C7\")\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_color(\"#BDC3C7\")\n",
    "    ax.spines[\"bottom\"].set_color(\"#BDC3C7\")\n",
    "    ax.spines[\"left\"].set_linewidth(1.5)\n",
    "    ax.spines[\"bottom\"].set_linewidth(1.5)\n",
    "    \n",
    "    # Format y-axis\n",
    "    def format_thousands(x, pos):\n",
    "        if x >= 1000000:\n",
    "            return f\"{x / 1000000:.1f}M\"\n",
    "        elif x >= 1000:\n",
    "            return f\"{x / 1000:.0f}K\"\n",
    "        else:\n",
    "            return f\"{x:.0f}\"\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
    "    ax.set_facecolor(\"#FAFAFA\")\n",
    "    \n",
    "    # Add value labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, \n",
    "                    labels=[format_thousands(v, None) if v > 0 else \"\" \n",
    "                           for v in container.datavalues],\n",
    "                    label_type=\"center\", fontsize=6, fontweight=\"bold\", color=\"black\")\n",
    "\n",
    "\n",
    "def _create_no_data_chart(ax, message: str) -> None:\n",
    "    \"\"\"Create chart for sections with no data.\"\"\"\n",
    "    ax.text(0.5, 0.5, f\"ðŸ“Š {message}\", transform=ax.transAxes, ha=\"center\", va=\"center\",\n",
    "           fontsize=12, fontweight=\"bold\", color=\"#7F8C8D\",\n",
    "           bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#ECF0F1\", \n",
    "                    edgecolor=\"#BDC3C7\", alpha=0.8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "\n",
    "def _add_global_legend(fig, all_sites: set) -> None:\n",
    "    \"\"\"Add global legend for all sites.\"\"\"\n",
    "    if all_sites:\n",
    "        from matplotlib.patches import Patch\n",
    "        \n",
    "        sorted_sites = sorted(all_sites)\n",
    "        legend_colors = [site_color_mapping[site] for site in sorted_sites]\n",
    "        legend_patches = [Patch(facecolor=color, alpha=0.85, edgecolor=\"white\") \n",
    "                         for color in legend_colors]\n",
    "        \n",
    "        global_legend = fig.legend(legend_patches, sorted_sites, bbox_to_anchor=(0.95, 0.5),\n",
    "                                 loc=\"center left\", fontsize=10, frameon=True, fancybox=True,\n",
    "                                 shadow=True, framealpha=0.9, facecolor=\"white\", \n",
    "                                 edgecolor=\"#BDC3C7\", title=\"Sitios\", title_fontsize=12)\n",
    "        global_legend.get_frame().set_linewidth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline for Excel file processing and analysis.\"\"\"\n",
    "    print(\"Starting Excel file processing pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: LOADING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        dfs = load_excel_files(XLSX_DATA_PATH)\n",
    "        \n",
    "        # Step 2: Clean data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: CLEANING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        dfs = clean_dataframes(dfs)\n",
    "        \n",
    "        # Step 3: Analyze data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: ANALYZING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        analyze_dataframes(dfs)\n",
    "        \n",
    "        # Step 4: Preview data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: PREVIEWING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        preview_dataframes_html(dfs)\n",
    "        display_datatypes(dfs)\n",
    "        \n",
    "        # Step 5: Process main DataFrame\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 5: PROCESSING MAIN DATAFRAME\")\n",
    "        print(\"=\"*60)\n",
    "        site_dfs = process_main_dataframe(dfs)\n",
    "        \n",
    "        # Step 6: Save processed data\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "        print(\"=\"*60)\n",
    "        save_sections_to_csv(site_dfs, OUTPUT_DIR)\n",
    "        \n",
    "        # Step 7: Create visualizations\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 7: CREATING VISUALIZATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        create_environmental_indicators_chart(site_dfs)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return dfs, site_dfs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Execute the main pipeline\n",
    "dfs, site_dfs = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
