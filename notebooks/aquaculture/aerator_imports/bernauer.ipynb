{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: LOADING DATA FROM ALL SHEETS\n",
      "============================================================\n",
      "Loading data from ../../../data/raw/aquaculture/aerator_imports/bernauer.xlsx\n",
      "\n",
      "Workbook: bernauer.xlsx\n",
      "Contains 5 sheets: å·¥ä½œè¡¨1, Sheet1, Sheet2, Sheet3, Sheet4\n",
      "\n",
      "  Sheet: 'å·¥ä½œè¡¨1'\n",
      "  Dimensions: 33 rows x 10 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Loaded df1 from sheet 'å·¥ä½œè¡¨1' with shape (33, 8)\n",
      "\n",
      "  Sheet: 'Sheet1'\n",
      "  Dimensions: 51 rows x 3 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 1\n",
      "  Loaded df2 from sheet 'Sheet1' with shape (50, 3)\n",
      "\n",
      "  Sheet: 'Sheet2'\n",
      "  Dimensions: 30 rows x 11 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Loaded df3 from sheet 'Sheet2' with shape (30, 11)\n",
      "\n",
      "  Sheet: 'Sheet3'\n",
      "  Dimensions: 4 rows x 7 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Detected header at row 1\n",
      "  Loaded df4 from sheet 'Sheet3' with shape (3, 7)\n",
      "\n",
      "  Sheet: 'Sheet4'\n",
      "  Dimensions: 42 rows x 10 columns\n",
      "  Note: Cannot check merged cells in read-only mode\n",
      "  Loaded df5 from sheet 'Sheet4' with shape (42, 10)\n",
      "\n",
      "âœ… Successfully loaded 5 DataFrames:\n",
      "  - df1: (33, 8)\n",
      "  - df2: (50, 3)\n",
      "  - df3: (30, 11)\n",
      "  - df4: (3, 7)\n",
      "  - df5: (42, 10)\n",
      "\n",
      "============================================================\n",
      "STEP 2: CLEANING ALL DATAFRAMES\n",
      "============================================================\n",
      "\n",
      "Cleaning df1...\n",
      "Starting data cleaning process...\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "Header standardization complete!\n",
      "Cleaned DataFrame: (33, 8) -> (26, 8)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for DataFrame:\n",
      "- Total NaN values: 159 (76.4% of all cells)\n",
      "- Removing 4 columns with >80.0% NaN values\n",
      "- Final shape after analysis: (26, 4)\n",
      "Data cleaning completed!\n",
      "\n",
      "Cleaning df2...\n",
      "Starting data cleaning process...\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "Header standardization complete!\n",
      "Cleaned DataFrame: (50, 3) -> (50, 3)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for DataFrame:\n",
      "- Total NaN values: 32 (21.3% of all cells)\n",
      "- Final shape after analysis: (50, 3)\n",
      "Data cleaning completed!\n",
      "\n",
      "Cleaning df3...\n",
      "Starting data cleaning process...\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "Header standardization complete!\n",
      "Cleaned DataFrame: (30, 11) -> (29, 8)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for DataFrame:\n",
      "- Total NaN values: 160 (69.0% of all cells)\n",
      "- Removing 5 columns with >80.0% NaN values\n",
      "- Final shape after analysis: (29, 3)\n",
      "Data cleaning completed!\n",
      "\n",
      "Cleaning df4...\n",
      "Starting data cleaning process...\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "Header standardization complete!\n",
      "Cleaned DataFrame: (3, 7) -> (3, 7)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for DataFrame:\n",
      "- Total NaN values: 3 (14.3% of all cells)\n",
      "- Final shape after analysis: (3, 7)\n",
      "Data cleaning completed!\n",
      "\n",
      "Cleaning df5...\n",
      "Starting data cleaning process...\n",
      "STANDARDIZING DATAFRAME HEADERS\n",
      "Header standardization complete!\n",
      "Cleaned DataFrame: (42, 10) -> (20, 5)\n",
      "\n",
      "--------------------------------------------------\n",
      "NaN analysis for DataFrame:\n",
      "- Total NaN values: 32 (32.0% of all cells)\n",
      "- Final shape after analysis: (20, 5)\n",
      "Data cleaning completed!\n",
      "\n",
      "âœ… Successfully cleaned 5 DataFrames\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import openpyxl\n",
    "from IPython.display import display, HTML\n",
    "from typing import Dict\n",
    "\n",
    "# Import project utilities\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "%matplotlib inline\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "XLSX_DATA_PATH = \"../../../data/raw/aquaculture/aerator_imports/bernauer.xlsx\"\n",
    "OUTPUT_DIR = \"../../../data/processed/bernauer/df2/\"\n",
    "\n",
    "# Global visualization settings\n",
    "used_colors = set()\n",
    "site_color_mapping = {}\n",
    "available_css4_colors = list(mcolors.CSS4_COLORS.keys())\n",
    "\n",
    "\n",
    "def load_all_excel_sheets(data_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all Excel sheets and return them as separate DataFrames.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path pattern for Excel files\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary of DataFrames keyed by sheet names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = glob.glob(data_path)\n",
    "        if not files:\n",
    "            print(\"No Excel files found matching the pattern.\")\n",
    "            return {}\n",
    "\n",
    "        all_dataframes = {}\n",
    "\n",
    "        for file in files:\n",
    "            print(f\"Loading data from {file}\")\n",
    "\n",
    "            # Load workbook with openpyxl for inspection\n",
    "            wb = openpyxl.load_workbook(file, data_only=True, read_only=True)\n",
    "            file_basename = os.path.basename(file)\n",
    "\n",
    "            print(f\"\\nWorkbook: {file_basename}\")\n",
    "            print(\n",
    "                f\"Contains {len(wb.sheetnames)} sheets: {', '.join(wb.sheetnames)}\"\n",
    "            )\n",
    "\n",
    "            # Process all sheets\n",
    "            for i, sheet_name in enumerate(wb.sheetnames):\n",
    "                df = _load_sheet_with_header_detection(\n",
    "                    file, sheet_name, wb[sheet_name]\n",
    "                )\n",
    "\n",
    "                if df is not None and not df.empty:\n",
    "                    df_key = f\"df{i + 1}\"\n",
    "                    all_dataframes[df_key] = df\n",
    "                    print(\n",
    "                        f\"  Loaded {df_key} from sheet '{sheet_name}' with shape {df.shape}\"\n",
    "                    )\n",
    "\n",
    "            wb.close()\n",
    "\n",
    "        return all_dataframes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def _load_sheet_with_header_detection(\n",
    "    file_path: str, sheet_name: str, sheet\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a sheet with automatic header detection.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file\n",
    "        sheet_name (str): Name of the sheet\n",
    "        sheet: Openpyxl sheet object\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame or None if sheet is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if sheet is empty\n",
    "        if not sheet.max_row or not sheet.max_column:\n",
    "            print(\n",
    "                f\"  Warning: Sheet '{sheet_name}' appears to be empty or corrupted\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        print(f\"\\n  Sheet: '{sheet_name}'\")\n",
    "        print(\n",
    "            f\"  Dimensions: {sheet.max_row} rows x {sheet.max_column} columns\"\n",
    "        )\n",
    "\n",
    "        # Check for merged cells\n",
    "        try:\n",
    "            merged_cells = list(sheet.merged_cells.ranges)\n",
    "            if merged_cells:\n",
    "                print(f\"  Contains {len(merged_cells)} merged cell ranges\")\n",
    "        except AttributeError:\n",
    "            print(\"  Note: Cannot check merged cells in read-only mode\")\n",
    "\n",
    "        # Load initial DataFrame without headers\n",
    "        df = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            header=None,\n",
    "            na_values=[\"NA\", \"N/A\", \"\"],\n",
    "            keep_default_na=True,\n",
    "        )\n",
    "\n",
    "        # Detect header row\n",
    "        header_row = _detect_header_row(df)\n",
    "\n",
    "        if header_row is not None:\n",
    "            # Reload with detected header\n",
    "            df = pd.read_excel(\n",
    "                file_path,\n",
    "                sheet_name=sheet_name,\n",
    "                header=header_row,\n",
    "                na_values=[\"NA\", \"N/A\", \"\"],\n",
    "                keep_default_na=True,\n",
    "            )\n",
    "            print(f\"  Detected header at row {header_row + 1}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading sheet '{sheet_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _detect_header_row(df: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to analyze\n",
    "\n",
    "    Returns:\n",
    "        int: Index of the header row, or None if not found\n",
    "    \"\"\"\n",
    "    for i in range(min(10, len(df))):\n",
    "        str_count = sum(1 for x in df.iloc[i] if isinstance(x, str))\n",
    "        if str_count > 0.5 * df.shape[1]:  # More than half are strings\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "# Step 1: Load all sheets\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1: LOADING DATA FROM ALL SHEETS\")\n",
    "print(\"=\" * 60)\n",
    "all_dataframes = load_all_excel_sheets(XLSX_DATA_PATH)\n",
    "\n",
    "if not all_dataframes:\n",
    "    print(\"âŒ No data loaded. Please check the file path and try again.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Extract individual DataFrames for easier access\n",
    "df1 = all_dataframes.get(\"df1\")\n",
    "df2 = all_dataframes.get(\"df2\")\n",
    "df3 = all_dataframes.get(\"df3\")\n",
    "df4 = all_dataframes.get(\"df4\")\n",
    "df5 = all_dataframes.get(\"df5\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully loaded {len(all_dataframes)} DataFrames:\")\n",
    "for df_name in all_dataframes.keys():\n",
    "    print(f\"  - {df_name}: {all_dataframes[df_name].shape}\")\n",
    "\n",
    "\n",
    "def clean_single_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning to a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to clean\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning process...\")\n",
    "\n",
    "    # Apply cleaning steps in order\n",
    "    df = standardize_headers_single(df)\n",
    "    df = remove_empty_rows_and_cols_single(df)\n",
    "    df = handle_nan_values_single(df)\n",
    "    df = title_case_columns_single(df)\n",
    "\n",
    "    print(\"Data cleaning completed!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_headers_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column headers for a single DataFrame.\"\"\"\n",
    "    print(\"STANDARDIZING DATAFRAME HEADERS\")\n",
    "\n",
    "    # Handle unnamed columns\n",
    "    df = _fix_unnamed_columns(df)\n",
    "\n",
    "    # Standardize date columns\n",
    "    df = _standardize_date_columns(df)\n",
    "\n",
    "    print(\"Header standardization complete!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_empty_rows_and_cols_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove completely empty rows and columns from DataFrame.\"\"\"\n",
    "    original_shape = df.shape\n",
    "    df = df.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    print(f\"Cleaned DataFrame: {original_shape} -> {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_nan_values_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Analyze and selectively handle NaN values in DataFrame.\"\"\"\n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(\"NaN analysis for DataFrame:\")\n",
    "\n",
    "    # Calculate NaN statistics\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    nan_percentage = (nan_count / total_cells) if total_cells > 0 else 0\n",
    "\n",
    "    print(\n",
    "        f\"- Total NaN values: {nan_count} ({nan_percentage:.1%} of all cells)\"\n",
    "    )\n",
    "\n",
    "    # Handle high-NaN columns\n",
    "    df = _remove_high_nan_columns(df)\n",
    "\n",
    "    # Handle critical row removal\n",
    "    df = _remove_critical_nan_rows(df)\n",
    "\n",
    "    print(f\"- Final shape after analysis: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def title_case_columns_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply title case to column names.\"\"\"\n",
    "    df.columns = df.columns.str.title()\n",
    "    return df\n",
    "\n",
    "\n",
    "def _fix_unnamed_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Fix unnamed columns by replacing them with meaningful names.\"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    for i, col in enumerate(columns):\n",
    "        col_str = str(col)\n",
    "        if col_str.startswith(\"Unnamed\") or col_str.isdigit():\n",
    "            columns[i] = f\"Column_{i}\"\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def _standardize_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize date column names.\"\"\"\n",
    "    date_column_mapping = {\n",
    "        \"fecha_embarque\": \"Fecha De Embarque\",\n",
    "        \"fecha_llegada\": \"Fecha De Llegada\",\n",
    "        \"fecha_ingreso\": \"Fecha Ingreso Sistema\",\n",
    "        \"fecha_pago\": \"Fecha Pago\",\n",
    "        \"fecha_liquidacion\": \"Fecha De LiquidaciÃ³n\",\n",
    "        \"fecha_aforo\": \"Fecha Aforo\",\n",
    "        \"fecha_salida\": \"Fecha Salida\",\n",
    "    }\n",
    "\n",
    "    # Apply mapping if columns exist, handling non-string column names\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        # Convert column to string if it's not already\n",
    "        col_str = str(col)\n",
    "        # Apply mapping\n",
    "        mapped_col = date_column_mapping.get(\n",
    "            col_str.lower().replace(\" \", \"_\"), col\n",
    "        )\n",
    "        new_columns.append(mapped_col)\n",
    "\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "\n",
    "def _remove_high_nan_columns(\n",
    "    df: pd.DataFrame, threshold: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Remove columns with high percentage of NaN values.\"\"\"\n",
    "    nan_percentage = df.isnull().sum() / len(df)\n",
    "    high_nan_cols = nan_percentage[nan_percentage > threshold].index.tolist()\n",
    "\n",
    "    if high_nan_cols:\n",
    "        print(\n",
    "            f\"- Removing {len(high_nan_cols)} columns with >{threshold * 100}% NaN values\"\n",
    "        )\n",
    "        df = df.drop(columns=high_nan_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _remove_critical_nan_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows where critical columns are all NaN.\"\"\"\n",
    "    critical_columns = [\n",
    "        \"Us$ Fob\",\n",
    "        \"Cantidad\",\n",
    "        \"Descripcion Producto Comercial\",\n",
    "    ]\n",
    "    existing_critical = [col for col in critical_columns if col in df.columns]\n",
    "\n",
    "    if existing_critical:\n",
    "        before_count = len(df)\n",
    "        df = df.dropna(subset=existing_critical, how=\"all\")\n",
    "        removed = before_count - len(df)\n",
    "        if removed > 0:\n",
    "            print(f\"- Removed {removed} rows with all critical columns as NaN\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _print_single_dataframe_info(name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print comprehensive information about a DataFrame.\"\"\"\n",
    "    print(f\"\\n{'-' * 50}\")\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Memory usage\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "\n",
    "    # Data types summary\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    print(f\"Data types: {dict(dtype_counts)}\")\n",
    "\n",
    "    # Missing values summary\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_cols = missing_summary[missing_summary > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "        print(f\"Total missing values: {missing_summary.sum()}\")\n",
    "    else:\n",
    "        print(\"No missing values found\")\n",
    "\n",
    "    print(f\"{'-' * 50}\")\n",
    "\n",
    "\n",
    "def clean_multiple_dataframes(\n",
    "    dataframes_dict: Dict[str, pd.DataFrame],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Apply cleaning to multiple DataFrames.\n",
    "\n",
    "    Args:\n",
    "        dataframes_dict (Dict[str, pd.DataFrame]): Dictionary of DataFrames to clean\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Dictionary of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    cleaned_dataframes = {}\n",
    "\n",
    "    for df_name, df in dataframes_dict.items():\n",
    "        print(f\"\\nCleaning {df_name}...\")\n",
    "        cleaned_df = clean_single_dataframe(df)\n",
    "        cleaned_dataframes[df_name] = cleaned_df\n",
    "\n",
    "    return cleaned_dataframes\n",
    "\n",
    "\n",
    "# Step 2: Clean all data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: CLEANING ALL DATAFRAMES\")\n",
    "print(\"=\" * 60)\n",
    "cleaned_dataframes = clean_multiple_dataframes(all_dataframes)\n",
    "\n",
    "# Update individual DataFrames with cleaned versions\n",
    "df1 = cleaned_dataframes.get(\"df1\")\n",
    "df2 = cleaned_dataframes.get(\"df2\")\n",
    "df3 = cleaned_dataframes.get(\"df3\")\n",
    "df4 = cleaned_dataframes.get(\"df4\")\n",
    "df5 = cleaned_dataframes.get(\"df5\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully cleaned {len(cleaned_dataframes)} DataFrames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: APPLYING ENHANCED CLEANING\n",
      "============================================================\n",
      "ðŸ”§ Cleaning df1 (Invoice Header)...\n",
      "   df1: (26, 4) -> (1, 10)\n",
      "ðŸ”§ Cleaning df2 (Product Catalog)...\n",
      "   df2: (50, 3) -> (50, 3)\n",
      "ðŸ”§ Cleaning df3...\n",
      "   df3: (29, 3) -> (28, 3)\n",
      "ðŸ”§ Cleaning df4 (Contact Info)...\n",
      "   df4: (3, 7) -> (3, 7)\n",
      "ðŸ”§ Cleaning df5 (Shipping Manifest)...\n",
      "   df5: (20, 5) -> (15, 5)\n",
      "\n",
      "âœ… Enhanced cleaning completed for 5 DataFrames\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š ENHANCED DATAFRAMES PREVIEW\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ ENHANCED_DF1:\n",
      "Shape: (1, 10)\n",
      "Columns: ['SOLD TO:', 'PI. NOï¼š', 'ACUICULTURA BER AQUA AQUIBER S.A.', 'DirecciÃ³n: JUNIN Y MARCEL LANIADO NÂº 439', 'MACHALA ECUADOR', 'MARK', 'UNIT PRICE', 'BR-3008LOP', 'TOTAL CIF', 'Payment:']\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SOLD TO:</th>\n",
       "      <th>PI. NOï¼š</th>\n",
       "      <th>ACUICULTURA BER AQUA AQUIBER S.A.</th>\n",
       "      <th>DirecciÃ³n: JUNIN Y MARCEL LANIADO NÂº 439</th>\n",
       "      <th>MACHALA ECUADOR</th>\n",
       "      <th>MARK</th>\n",
       "      <th>UNIT PRICE</th>\n",
       "      <th>BR-3008LOP</th>\n",
       "      <th>TOTAL CIF</th>\n",
       "      <th>Payment:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>BRAC01</td>\n",
       "      <td>BR-AC202502801</td>\n",
       "      <td>PI. DATEï¼š</td>\n",
       "      <td>P.O.E.ï¼š</td>\n",
       "      <td>P.O.D.ï¼š</td>\n",
       "      <td>MODEL</td>\n",
       "      <td>AMOUNT(US$)</td>\n",
       "      <td>514</td>\n",
       "      <td>5140</td>\n",
       "      <td>30% downpay 70 %T/T in advance against  B/L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "ðŸ“‹ ENHANCED_DF2:\n",
      "Shape: (50, 3)\n",
      "Columns: ['Model_', 'Descirption', 'Column_2']\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model_</th>\n",
       "      <th>Descirption</th>\n",
       "      <th>Column_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>YH-SSGP</td>\n",
       "      <td>Small Spiral Bevel Gear for Plastic Reducer</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-LSGP</td>\n",
       "      <td>Large Spiral Bevel Gear</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-SHSP</td>\n",
       "      <td>Small Helical Shaft</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "ðŸ“‹ ENHANCED_DF3:\n",
      "Shape: (28, 3)\n",
      "Columns: ['Column_4', 'Column_9', 'Column_10']\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Column_4</th>\n",
       "      <th>Column_9</th>\n",
       "      <th>Column_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FOURTY</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>3.0</td>\n",
       "      <td>THREE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "ðŸ“‹ ENHANCED_DF4:\n",
      "Shape: (3, 7)\n",
      "Columns: ['Customer_ID', 'Company_Name', 'Contact_Person', 'Address', 'Location', 'Phone', 'City']\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Contact_Person</th>\n",
       "      <th>Address</th>\n",
       "      <th>Location</th>\n",
       "      <th>Phone</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ECEC01</td>\n",
       "      <td>IMPORTARA ECUASINO ., SA</td>\n",
       "      <td>ATTN: Mr  Yu</td>\n",
       "      <td>Centro Empresarial Colon-Empresarial 3- of. 210</td>\n",
       "      <td>ECUADOR</td>\n",
       "      <td>Phone : 593-9484985</td>\n",
       "      <td>GUAYAQUIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BRBA01</td>\n",
       "      <td>Bernadette Aquacultura LTD</td>\n",
       "      <td>ATTN: Mr Laurent Percy Fleury</td>\n",
       "      <td>Caixa Postal 61 indaial Santa Catarina</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BRAC01</td>\n",
       "      <td>ACUICULTURA BER AQUA AQUIBER S.A.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DirecciÃ³n: JUNIN Y MARCEL LANIADO NÂº 439</td>\n",
       "      <td>MACHALA ECUADOR</td>\n",
       "      <td>59072935953</td>\n",
       "      <td>GUAYAQUIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\n",
      "ðŸ“‹ ENHANCED_DF5:\n",
      "Shape: (15, 5)\n",
      "Columns: ['Teffer_Float', '0.0945', '250', '23.625', 'Column_4']\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Teffer_Float</th>\n",
       "      <th>0.0945</th>\n",
       "      <th>250</th>\n",
       "      <th>23.625</th>\n",
       "      <th>Column_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>YiYuan Floats</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>258</td>\n",
       "      <td>24.381</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Teffer impeller</td>\n",
       "      <td>0.032</td>\n",
       "      <td>600</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YiYuan Plastic Reducer with Mechanical Seal</td>\n",
       "      <td>0.02025</td>\n",
       "      <td>300</td>\n",
       "      <td>6.075</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def detect_data_structure(df: pd.DataFrame) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze DataFrame structure to identify header rows, data sections, and metadata.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw DataFrame to analyze\n",
    "\n",
    "    Returns:\n",
    "        Dict containing structure information\n",
    "    \"\"\"\n",
    "    structure_info = {\n",
    "        \"header_row\": None,\n",
    "        \"data_start_row\": None,\n",
    "        \"metadata_rows\": [],\n",
    "        \"empty_rows\": [],\n",
    "        \"likely_headers\": [],\n",
    "        \"data_type\": \"unknown\",\n",
    "    }\n",
    "\n",
    "    # Find empty rows\n",
    "    for idx, row in df.iterrows():\n",
    "        if row.isna().all():\n",
    "            structure_info[\"empty_rows\"].append(idx)\n",
    "\n",
    "    # Look for potential header patterns\n",
    "    for idx, row in df.iterrows():\n",
    "        non_null_values = row.dropna().values\n",
    "        if len(non_null_values) > 0:\n",
    "            # Check if row contains typical header keywords\n",
    "            header_keywords = [\n",
    "                \"model\",\n",
    "                \"description\",\n",
    "                \"price\",\n",
    "                \"amount\",\n",
    "                \"qty\",\n",
    "                \"quantity\",\n",
    "                \"unit\",\n",
    "                \"mark\",\n",
    "                \"id\",\n",
    "                \"name\",\n",
    "                \"address\",\n",
    "                \"tel\",\n",
    "                \"email\",\n",
    "            ]\n",
    "\n",
    "            row_text = \" \".join(\n",
    "                [str(val).lower() for val in non_null_values if pd.notna(val)]\n",
    "            )\n",
    "\n",
    "            if any(keyword in row_text for keyword in header_keywords):\n",
    "                structure_info[\"likely_headers\"].append(idx)\n",
    "\n",
    "    # Determine data type based on content\n",
    "    all_text = \" \".join(\n",
    "        [str(val).lower() for val in df.values.flatten() if pd.notna(val)]\n",
    "    )\n",
    "\n",
    "    if any(\n",
    "        term in all_text for term in [\"model\", \"price\", \"unit price\", \"amount\"]\n",
    "    ):\n",
    "        structure_info[\"data_type\"] = \"product_catalog\"\n",
    "    elif any(\n",
    "        term in all_text for term in [\"address\", \"tel\", \"email\", \"company\"]\n",
    "    ):\n",
    "        structure_info[\"data_type\"] = \"contact_info\"\n",
    "    elif any(term in all_text for term in [\"invoice\", \"pi. no\", \"sold to\"]):\n",
    "        structure_info[\"data_type\"] = \"invoice_header\"\n",
    "    elif any(\n",
    "        term in all_text\n",
    "        for term in [\"cubit meter\", \"qty\", \"float\", \"impeller\"]\n",
    "    ):\n",
    "        structure_info[\"data_type\"] = \"shipping_manifest\"\n",
    "\n",
    "    return structure_info\n",
    "\n",
    "\n",
    "def clean_product_catalog(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean product catalog data (like df2).\"\"\"\n",
    "    # Find the actual header row\n",
    "    header_row_idx = None\n",
    "    for idx, row in df.iterrows():\n",
    "        if any(\n",
    "            \"model\" in str(val).lower() for val in row.values if pd.notna(val)\n",
    "        ):\n",
    "            header_row_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_row_idx is not None:\n",
    "        # Set proper headers\n",
    "        new_headers = df.iloc[header_row_idx].values\n",
    "        df_clean = df.iloc[header_row_idx + 1 :].copy()\n",
    "        df_clean.columns = [\n",
    "            str(col).strip() if pd.notna(col) else f\"Column_{i}\"\n",
    "            for i, col in enumerate(new_headers)\n",
    "        ]\n",
    "    else:\n",
    "        df_clean = df.copy()\n",
    "\n",
    "    # Clean column names\n",
    "    df_clean.columns = [\n",
    "        col.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        for col in df_clean.columns\n",
    "    ]\n",
    "\n",
    "    # Remove rows that are all NaN\n",
    "    df_clean = df_clean.dropna(how=\"all\")\n",
    "\n",
    "    # Convert price columns to numeric\n",
    "    for col in df_clean.columns:\n",
    "        if (\n",
    "            \"price\" in col.lower()\n",
    "            or \"amount\" in col.lower()\n",
    "            or col.startswith(\"Column_\")\n",
    "        ):\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "    # Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def clean_contact_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean contact information data (like df4).\"\"\"\n",
    "    # Remove rows that are mostly empty\n",
    "    df_clean = df.dropna(\n",
    "        thresh=3\n",
    "    ).copy()  # Keep rows with at least 3 non-null values\n",
    "\n",
    "    # Try to create meaningful column names based on content\n",
    "    if len(df_clean) > 0:\n",
    "        potential_columns = [\n",
    "            \"Customer_ID\",\n",
    "            \"Company_Name\",\n",
    "            \"Contact_Person\",\n",
    "            \"Address\",\n",
    "            \"Location\",\n",
    "            \"Phone\",\n",
    "            \"City\",\n",
    "        ]\n",
    "\n",
    "        # Assign column names based on available columns\n",
    "        new_columns = []\n",
    "        for i, col in enumerate(df_clean.columns):\n",
    "            if i < len(potential_columns):\n",
    "                new_columns.append(potential_columns[i])\n",
    "            else:\n",
    "                new_columns.append(f\"Additional_Info_{i}\")\n",
    "\n",
    "        df_clean.columns = new_columns[: len(df_clean.columns)]\n",
    "\n",
    "    # Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def clean_invoice_header(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean invoice header data (like df1).\"\"\"\n",
    "    # This type of data is often metadata - extract key-value pairs\n",
    "    invoice_data = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_values = [\n",
    "            str(val)\n",
    "            for val in row.values\n",
    "            if pd.notna(val) and str(val).strip()\n",
    "        ]\n",
    "\n",
    "        if len(row_values) >= 2:\n",
    "            # Look for key-value patterns\n",
    "            for i in range(0, len(row_values) - 1, 2):\n",
    "                key = str(row_values[i]).strip()\n",
    "                value = (\n",
    "                    str(row_values[i + 1]).strip()\n",
    "                    if i + 1 < len(row_values)\n",
    "                    else \"\"\n",
    "                )\n",
    "\n",
    "                if key and value and key not in [\"nan\", \"NaN\"]:\n",
    "                    invoice_data[key] = value\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if invoice_data:\n",
    "        df_clean = pd.DataFrame([invoice_data])\n",
    "    else:\n",
    "        # Fallback: just clean the original structure\n",
    "        df_clean = df.dropna(how=\"all\").reset_index(drop=True)\n",
    "        # Remove columns that are mostly empty\n",
    "        for col in df_clean.columns:\n",
    "            if df_clean[col].isna().sum() / len(df_clean) > 0.8:\n",
    "                df_clean = df_clean.drop(columns=[col])\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def clean_shipping_manifest(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean shipping manifest data (like df5).\"\"\"\n",
    "    # Find header row\n",
    "    header_row_idx = None\n",
    "    for idx, row in df.iterrows():\n",
    "        if any(\n",
    "            \"qty\" in str(val).lower() for val in row.values if pd.notna(val)\n",
    "        ):\n",
    "            header_row_idx = idx\n",
    "            break\n",
    "\n",
    "    if header_row_idx is not None:\n",
    "        # Set proper headers\n",
    "        headers = df.iloc[header_row_idx].values\n",
    "        df_clean = df.iloc[header_row_idx + 1 :].copy()\n",
    "\n",
    "        # Create meaningful column names\n",
    "        new_columns = []\n",
    "        for i, header in enumerate(headers):\n",
    "            if pd.notna(header) and str(header).strip():\n",
    "                new_columns.append(str(header).strip().replace(\" \", \"_\"))\n",
    "            else:\n",
    "                # Infer from data pattern\n",
    "                if i == 0:\n",
    "                    new_columns.append(\"Product_Name\")\n",
    "                elif \"cubit\" in str(headers).lower():\n",
    "                    new_columns.append(f\"Volume_m3_{i}\")\n",
    "                elif any(\n",
    "                    \"qty\" in str(h).lower() for h in headers if pd.notna(h)\n",
    "                ):\n",
    "                    new_columns.append(\"Quantity\")\n",
    "                else:\n",
    "                    new_columns.append(f\"Column_{i}\")\n",
    "\n",
    "        df_clean.columns = new_columns\n",
    "    else:\n",
    "        df_clean = df.copy()\n",
    "        # Create default meaningful names\n",
    "        df_clean.columns = [\n",
    "            \"Product_Name\",\n",
    "            \"Unit_Volume_m3\",\n",
    "            \"Quantity\",\n",
    "            \"Total_Volume_m3\",\n",
    "            \"Notes\",\n",
    "        ][: len(df_clean.columns)]\n",
    "\n",
    "    # Clean data types\n",
    "    for col in df_clean.columns:\n",
    "        if (\n",
    "            \"volume\" in col.lower()\n",
    "            or \"qty\" in col.lower()\n",
    "            or \"quantity\" in col.lower()\n",
    "        ):\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "    # Remove empty rows and rows with no product name\n",
    "    df_clean = df_clean.dropna(subset=[df_clean.columns[0]], how=\"all\")\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def basic_dataframe_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Basic cleaning for unrecognized data types.\"\"\"\n",
    "    # Remove completely empty rows and columns\n",
    "    df_clean = df.dropna(how=\"all\").copy()\n",
    "    df_clean = df_clean.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Remove columns that are >90% empty\n",
    "    threshold = len(df_clean) * 0.1  # Keep if at least 10% has data\n",
    "    df_clean = df_clean.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "    # Clean column names\n",
    "    df_clean.columns = [\n",
    "        f\"Column_{i}\"\n",
    "        if str(col).strip() == \"\" or pd.isna(col)\n",
    "        else str(col).strip()\n",
    "        for i, col in enumerate(df_clean.columns)\n",
    "    ]\n",
    "\n",
    "    return df_clean.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Apply enhanced cleaning to all available DataFrames\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: APPLYING ENHANCED CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "enhanced_dataframes = {}\n",
    "\n",
    "if df1 is not None:\n",
    "    print(\"ðŸ”§ Cleaning df1 (Invoice Header)...\")\n",
    "    enhanced_df1 = clean_invoice_header(df1)\n",
    "    enhanced_dataframes[\"enhanced_df1\"] = enhanced_df1\n",
    "    print(f\"   df1: {df1.shape} -> {enhanced_df1.shape}\")\n",
    "\n",
    "if df2 is not None:\n",
    "    print(\"ðŸ”§ Cleaning df2 (Product Catalog)...\")\n",
    "    enhanced_df2 = clean_product_catalog(df2)\n",
    "    enhanced_dataframes[\"enhanced_df2\"] = enhanced_df2\n",
    "    print(f\"   df2: {df2.shape} -> {enhanced_df2.shape}\")\n",
    "\n",
    "if df3 is not None:\n",
    "    print(\"ðŸ”§ Cleaning df3...\")\n",
    "    enhanced_df3 = basic_dataframe_clean(df3)\n",
    "    enhanced_dataframes[\"enhanced_df3\"] = enhanced_df3\n",
    "    print(f\"   df3: {df3.shape} -> {enhanced_df3.shape}\")\n",
    "\n",
    "if df4 is not None:\n",
    "    print(\"ðŸ”§ Cleaning df4 (Contact Info)...\")\n",
    "    enhanced_df4 = clean_contact_info(df4)\n",
    "    enhanced_dataframes[\"enhanced_df4\"] = enhanced_df4\n",
    "    print(f\"   df4: {df4.shape} -> {enhanced_df4.shape}\")\n",
    "\n",
    "if df5 is not None:\n",
    "    print(\"ðŸ”§ Cleaning df5 (Shipping Manifest)...\")\n",
    "    enhanced_df5 = clean_shipping_manifest(df5)\n",
    "    enhanced_dataframes[\"enhanced_df5\"] = enhanced_df5\n",
    "    print(f\"   df5: {df5.shape} -> {enhanced_df5.shape}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nâœ… Enhanced cleaning completed for {len(enhanced_dataframes)} DataFrames\"\n",
    ")\n",
    "\n",
    "# Preview each enhanced dataframe\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸ“Š ENHANCED DATAFRAMES PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for df_name, df in enhanced_dataframes.items():\n",
    "    print(f\"\\nðŸ“‹ {df_name.upper()}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    if not df.empty:\n",
    "        print(\"Columns:\", list(df.columns))\n",
    "        print(\"Preview:\")\n",
    "        display(HTML(df.head(3).to_html(index=False)))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ ANALYZING DF2: PRODUCT CATALOG\n",
      "==================================================\n",
      "Original shape: (50, 3)\n",
      "Columns after standardization: ['Code', 'Description', 'Cost']\n",
      "\n",
      "Rows with valid cost data: 18 out of 50\n",
      "\n",
      "ðŸ“Š CODE COST ANALYSIS (Top 10):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109581/4144877237.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_with_cost[\"Manufacturer\"] = df2_with_cost[\"Code\"].str[:2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Code</th>\n",
       "      <th>Total_Cost</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>BR-3008LOP</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-2004LMP</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-FEEDERL</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TF-2004LMP</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EC-MR5.5MN</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-2HPMO</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-1HPMOS</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-PRD</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TF-PRD</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>YH-FMOL</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š MANUFACTURER COST ANALYSIS:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Total_Cost</th>\n",
       "      <th>Component_Count</th>\n",
       "      <th>Manufacturer_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>YH</td>\n",
       "      <td>883.3</td>\n",
       "      <td>13</td>\n",
       "      <td>YIYUAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BR</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1</td>\n",
       "      <td>BERAQUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TF</td>\n",
       "      <td>334.0</td>\n",
       "      <td>3</td>\n",
       "      <td>TEFFER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EC</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ECUASINO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ SUMMARY STATISTICS:\n",
      "Total manufacturers: 4\n",
      "Total unique codes: 18\n",
      "Total cost across all codes: $1998.30\n",
      "Highest cost code: BR-3008LOP ($514.00)\n",
      "Highest cost manufacturer: YIYUAN ($883.30)\n"
     ]
    }
   ],
   "source": [
    "# Analyze df2 by standardizing column names and grouping by code\n",
    "print(\"ðŸ”§ ANALYZING DF2: PRODUCT CATALOG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a copy and standardize column names\n",
    "df2_analysis = enhanced_df2.copy()\n",
    "df2_analysis.columns = [\"Code\", \"Description\", \"Cost\"]\n",
    "\n",
    "print(f\"Original shape: {df2_analysis.shape}\")\n",
    "print(f\"Columns after standardization: {list(df2_analysis.columns)}\")\n",
    "\n",
    "# Convert Cost column to numeric, handling any non-numeric values\n",
    "df2_analysis[\"Cost\"] = pd.to_numeric(df2_analysis[\"Cost\"], errors=\"coerce\")\n",
    "\n",
    "# Remove rows where Cost is NaN for grouping analysis\n",
    "df2_with_cost = df2_analysis.dropna(subset=[\"Cost\"])\n",
    "\n",
    "print(\n",
    "    f\"\\nRows with valid cost data: {len(df2_with_cost)} out of {len(df2_analysis)}\"\n",
    ")\n",
    "\n",
    "# Extract manufacturer from code (first 2 letters)\n",
    "df2_with_cost[\"Manufacturer\"] = df2_with_cost[\"Code\"].str[:2]\n",
    "\n",
    "# Define manufacturer mapping\n",
    "manufacturer_names = {\n",
    "    \"BR\": \"BERAQUA\",\n",
    "    \"EC\": \"ECUASINO\",\n",
    "    \"TF\": \"TEFFER\",\n",
    "    \"YH\": \"YIYUAN\",\n",
    "}\n",
    "\n",
    "# Group by Code and sum costs\n",
    "code_totals = (\n",
    "    df2_with_cost.groupby(\"Code\")[\"Cost\"].agg([\"sum\", \"count\"]).reset_index()\n",
    ")\n",
    "code_totals.columns = [\"Code\", \"Total_Cost\", \"Count\"]\n",
    "\n",
    "# Sort by Total_Cost in descending order\n",
    "code_totals_sorted = code_totals.sort_values(\"Total_Cost\", ascending=False)\n",
    "\n",
    "# Group by manufacturer\n",
    "manufacturer_totals = (\n",
    "    df2_with_cost.groupby(\"Manufacturer\")[\"Cost\"]\n",
    "    .agg([\"sum\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "manufacturer_totals.columns = [\"Manufacturer\", \"Total_Cost\", \"Component_Count\"]\n",
    "manufacturer_totals[\"Manufacturer_Name\"] = manufacturer_totals[\n",
    "    \"Manufacturer\"\n",
    "].map(manufacturer_names)\n",
    "manufacturer_totals_sorted = manufacturer_totals.sort_values(\n",
    "    \"Total_Cost\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š CODE COST ANALYSIS (Top 10):\")\n",
    "print(\"-\" * 40)\n",
    "display(HTML(code_totals_sorted.head(10).to_html(index=False)))\n",
    "\n",
    "print(\"\\nðŸ“Š MANUFACTURER COST ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "display(HTML(manufacturer_totals_sorted.to_html(index=False)))\n",
    "\n",
    "print(\"\\nðŸ“ˆ SUMMARY STATISTICS:\")\n",
    "print(f\"Total manufacturers: {len(manufacturer_totals)}\")\n",
    "print(f\"Total unique codes: {len(code_totals)}\")\n",
    "print(f\"Total cost across all codes: ${code_totals['Total_Cost'].sum():.2f}\")\n",
    "print(\n",
    "    f\"Highest cost code: {code_totals_sorted.iloc[0]['Code']} (${code_totals_sorted.iloc[0]['Total_Cost']:.2f})\"\n",
    ")\n",
    "print(\n",
    "    f\"Highest cost manufacturer: {manufacturer_totals_sorted.iloc[0]['Manufacturer_Name']} (${manufacturer_totals_sorted.iloc[0]['Total_Cost']:.2f})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Creating Manufacturer Summary sheet...\n",
      "ðŸ“Š Creating Model Summary sheet...\n",
      "ðŸ“Š Creating Insights sheet...\n",
      "ðŸ“Š Creating All Components List sheet...\n",
      "ðŸ“Š Creating manufacturer breakdown sheets...\n",
      "   âœ“ YH (YIYUAN): 13 components, Total: $883.30\n",
      "   âœ“ BR (BERAQUA): 1 components, Total: $514.00\n",
      "   âœ“ TF (TEFFER): 3 components, Total: $334.00\n",
      "   âœ“ EC (ECUASINO): 1 components, Total: $267.00\n",
      "\n",
      "âœ… Comprehensive analysis exported to: ../../../data/processed/bernauer/df2/bernauer_cost_analysis.xlsx\n",
      "ðŸ“‹ Excel file contains:\n",
      "   â€¢ Manufacturer_Summary sheet: Overall manufacturer cost summary\n",
      "   â€¢ Model_Summary sheet: Summary by individual model code\n",
      "   â€¢ Insights sheet: Key metrics and statistics\n",
      "   â€¢ All_Components: Complete list of all components sorted by manufacturer\n",
      "   â€¢ 4 manufacturer sheets: Detailed component breakdowns for each manufacturer\n",
      "   â€¢ Manufacturer summary: ../../../data/processed/bernauer/df2/manufacturer_totals.csv\n",
      "   â€¢ Model summary: ../../../data/processed/bernauer/df2/model_totals.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109581/2568440404.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_with_cost[\"Manufacturer\"] = df2_with_cost[\"Code\"].str[:2]\n"
     ]
    }
   ],
   "source": [
    "# Export the model totals to the output directory create directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Comprehensive Excel export\n",
    "excel_output_file = os.path.join(OUTPUT_DIR, \"bernauer_cost_analysis.xlsx\")\n",
    "\n",
    "# Extract manufacturer from model code (first 2 letters)\n",
    "df2_with_cost[\"Manufacturer\"] = df2_with_cost[\"Code\"].str[:2]\n",
    "\n",
    "# Get mapping from manufacturer code to full name\n",
    "manufacturer_names = {\n",
    "    \"BR\": \"BERAQUA\",\n",
    "    \"EC\": \"ECUASINO\",\n",
    "    \"TF\": \"TEFFER\",\n",
    "    \"YH\": \"YIYUAN\",\n",
    "}\n",
    "\n",
    "# Group by manufacturer\n",
    "manufacturer_totals = (\n",
    "    df2_with_cost.groupby(\"Manufacturer\")[\"Cost\"]\n",
    "    .agg([\"sum\", \"count\"])\n",
    "    .reset_index()\n",
    ")\n",
    "manufacturer_totals.columns = [\"Manufacturer\", \"Total_Cost\", \"Component_Count\"]\n",
    "\n",
    "# Add full manufacturer name\n",
    "manufacturer_totals[\"Manufacturer_Name\"] = manufacturer_totals[\n",
    "    \"Manufacturer\"\n",
    "].map(manufacturer_names)\n",
    "\n",
    "# Sort by Total_Cost in descending order\n",
    "manufacturer_totals_sorted = manufacturer_totals.sort_values(\n",
    "    \"Total_Cost\", ascending=False\n",
    ")\n",
    "\n",
    "\n",
    "# Function to prepare detailed manufacturer breakdown from actual component data\n",
    "def prepare_manufacturer_breakdown(manufacturer_code, df_components):\n",
    "    \"\"\"\n",
    "    Prepare a detailed breakdown for a manufacturer using actual component data.\n",
    "\n",
    "    Args:\n",
    "        manufacturer_code (str): The code of the manufacturer (BR, EC, TF, YH)\n",
    "        df_components (DataFrame): DataFrame containing manufacturer component data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Enhanced manufacturer breakdown with components and summary\n",
    "    \"\"\"\n",
    "    # Sort by Cost in descending order\n",
    "    df_sorted = df_components.sort_values(\"Cost\", ascending=False).copy()\n",
    "\n",
    "    # Calculate total cost\n",
    "    total_cost = df_sorted[\"Cost\"].sum()\n",
    "\n",
    "    # Add percentage column showing contribution to total manufacturer cost\n",
    "    df_sorted[\"Percentage\"] = (df_sorted[\"Cost\"] / total_cost * 100).round(2)\n",
    "    df_sorted[\"Percentage\"] = df_sorted[\"Percentage\"].astype(str) + \"%\"\n",
    "\n",
    "    # Add summary row\n",
    "    manufacturer_name = manufacturer_names.get(\n",
    "        manufacturer_code, manufacturer_code\n",
    "    )\n",
    "    summary_row = pd.DataFrame(\n",
    "        {\n",
    "            \"Code\": [\"TOTAL\"],\n",
    "            \"Description\": [f\"Total for {manufacturer_name}\"],\n",
    "            \"Cost\": [total_cost],\n",
    "            \"Percentage\": [\"100.00%\"],\n",
    "            \"Manufacturer\": [manufacturer_code],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Combine components and summary\n",
    "    df_combined = pd.concat([df_sorted, summary_row], ignore_index=True)\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(excel_output_file, engine=\"openpyxl\") as writer:\n",
    "    # Sheet 1: Summary by Manufacturer\n",
    "    print(\"ðŸ“Š Creating Manufacturer Summary sheet...\")\n",
    "    manufacturer_totals_sorted.to_excel(\n",
    "        writer, sheet_name=\"Manufacturer_Summary\", index=False\n",
    "    )\n",
    "\n",
    "    # Sheet 2: All Models Summary\n",
    "    print(\"ðŸ“Š Creating Model Summary sheet...\")\n",
    "    # Group by individual model code\n",
    "    model_totals = (\n",
    "        df2_with_cost.groupby(\"Code\")[\"Cost\"]\n",
    "        .agg([\"sum\", \"count\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    model_totals.columns = [\"Code\", \"Total_Cost\", \"Component_Count\"]\n",
    "    model_totals_sorted = model_totals.sort_values(\n",
    "        \"Total_Cost\", ascending=False\n",
    "    )\n",
    "    model_totals_sorted.to_excel(\n",
    "        writer, sheet_name=\"Model_Summary\", index=False\n",
    "    )\n",
    "\n",
    "    # Sheet 3: Detailed Insights\n",
    "    print(\"ðŸ“Š Creating Insights sheet...\")\n",
    "    insights_data = {\n",
    "        \"Metric\": [\n",
    "            \"Total Manufacturers\",\n",
    "            \"Total Model Codes\",\n",
    "            \"Total Components Analyzed\",\n",
    "            \"Components with Cost Data\",\n",
    "            \"Total Cost (USD)\",\n",
    "            \"Highest Cost Manufacturer\",\n",
    "            \"Highest Cost Manufacturer Amount (USD)\",\n",
    "            \"Lowest Cost Manufacturer\",\n",
    "            \"Lowest Cost Manufacturer Amount (USD)\",\n",
    "            \"Highest Cost Component\",\n",
    "            \"Highest Cost Component Amount (USD)\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            len(manufacturer_totals),\n",
    "            len(model_totals),\n",
    "            len(df2_analysis),\n",
    "            len(df2_with_cost),\n",
    "            f\"${df2_with_cost['Cost'].sum():.2f}\",\n",
    "            manufacturer_totals_sorted.iloc[0][\"Manufacturer_Name\"],\n",
    "            f\"${manufacturer_totals_sorted.iloc[0]['Total_Cost']:.2f}\",\n",
    "            manufacturer_totals_sorted.iloc[-1][\"Manufacturer_Name\"],\n",
    "            f\"${manufacturer_totals_sorted.iloc[-1]['Total_Cost']:.2f}\",\n",
    "            df2_with_cost.sort_values(\"Cost\", ascending=False).iloc[0][\"Code\"],\n",
    "            f\"${df2_with_cost.sort_values('Cost', ascending=False).iloc[0]['Cost']:.2f}\",\n",
    "        ],\n",
    "    }\n",
    "    insights_df = pd.DataFrame(insights_data)\n",
    "    insights_df.to_excel(writer, sheet_name=\"Insights\", index=False)\n",
    "\n",
    "    # Sheet 4: All Components List\n",
    "    print(\"ðŸ“Š Creating All Components List sheet...\")\n",
    "    # Sort by Manufacturer then by Cost\n",
    "    all_components_df = df2_with_cost.sort_values(\n",
    "        [\"Manufacturer\", \"Cost\"], ascending=[True, False]\n",
    "    )\n",
    "    all_components_df.to_excel(\n",
    "        writer, sheet_name=\"All_Components\", index=False\n",
    "    )\n",
    "\n",
    "    # Individual sheets for each manufacturer with detailed breakdowns\n",
    "    print(\"ðŸ“Š Creating manufacturer breakdown sheets...\")\n",
    "\n",
    "    # Process each manufacturer\n",
    "    for i, manufacturer_row in enumerate(\n",
    "        manufacturer_totals_sorted.to_dict(\"records\"), 1\n",
    "    ):\n",
    "        manufacturer_code = manufacturer_row[\"Manufacturer\"]\n",
    "        manufacturer_name = manufacturer_row[\"Manufacturer_Name\"]\n",
    "\n",
    "        # Get components for this manufacturer from the dataset\n",
    "        manufacturer_components = df2_with_cost[\n",
    "            df2_with_cost[\"Manufacturer\"] == manufacturer_code\n",
    "        ].copy()\n",
    "\n",
    "        # Prepare detailed breakdown\n",
    "        manufacturer_detail_df = prepare_manufacturer_breakdown(\n",
    "            manufacturer_code, manufacturer_components\n",
    "        )\n",
    "\n",
    "        # Clean sheet name for Excel\n",
    "        sheet_name = f\"{manufacturer_name}\"\n",
    "\n",
    "        # Write to Excel\n",
    "        manufacturer_detail_df.to_excel(\n",
    "            writer, sheet_name=sheet_name, index=False\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"   âœ“ {manufacturer_code} ({manufacturer_name}): {len(manufacturer_components)} components, Total: ${manufacturer_row['Total_Cost']:.2f}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nâœ… Comprehensive analysis exported to: {excel_output_file}\")\n",
    "print(\"ðŸ“‹ Excel file contains:\")\n",
    "print(\"   â€¢ Manufacturer_Summary sheet: Overall manufacturer cost summary\")\n",
    "print(\"   â€¢ Model_Summary sheet: Summary by individual model code\")\n",
    "print(\"   â€¢ Insights sheet: Key metrics and statistics\")\n",
    "print(\n",
    "    \"   â€¢ All_Components: Complete list of all components sorted by manufacturer\"\n",
    ")\n",
    "print(\n",
    "    f\"   â€¢ {len(manufacturer_totals)} manufacturer sheets: Detailed component breakdowns for each manufacturer\"\n",
    ")\n",
    "\n",
    "# Also export CSV summaries for quick reference\n",
    "manufacturer_csv = os.path.join(OUTPUT_DIR, \"manufacturer_totals.csv\")\n",
    "manufacturer_totals_sorted.to_csv(manufacturer_csv, index=False)\n",
    "model_csv = os.path.join(OUTPUT_DIR, \"model_totals.csv\")\n",
    "model_totals_sorted.to_csv(model_csv, index=False)\n",
    "print(f\"   â€¢ Manufacturer summary: {manufacturer_csv}\")\n",
    "print(f\"   â€¢ Model summary: {model_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
